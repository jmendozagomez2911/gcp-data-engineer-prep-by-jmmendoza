# ğŸš€ğŸ“¡ Module â€” Streaming Ingestion Foundations (Pub/Sub vs Managed Kafka) + Architectures + Lab (Deep Dive)

**Goal:** Pick the right **streaming ingestion layer** (Pub/Sub vs Managed Service for Apache Kafka), understand **what problems each one is designed to solve**, and know the **architectural + reliability knobs** that the **GCP Data Engineer exam** tries to catch you with.

**How to read:**

1. Why streaming â†’ 2) Use cases & reference architectures â†’ 3) Pub/Sub core model â†’ 4) Pub/Sub delivery/reliability & integrations â†’ 5) Kafka model & what it buys you â†’ 6) Choosing + hybrid (Kafka Connect) â†’ 7) Lab: what it teaches + why errors happened â†’ 8) Exam cheats + quiz answers explained.

---

## 1) ğŸ§  Why streaming exists (first principles)

Streaming is not â€œbatch but faster.â€ Itâ€™s a different operating mode:

* In **batch**, data is *at rest* (you process a file/table at a moment in time).
* In **streaming**, data is *in motion* (events keep arriving and you must keep up continuously).

So the real streaming problem is always:

### âœ… â€œCan you ingest and process without falling behind?â€

Thatâ€™s why the module highlights:

* **High velocity/volume**: if you canâ€™t absorb bursts, you build backlog.
* **Low latency**: you need *fresh state* quickly (leaderboards, anti-cheat).
* **Ordering + correctness**: streams donâ€™t arrive nicely sorted.
* **Elastic scaling**: load spikes must auto-scale.
* **Fault tolerance**: losing events is unacceptable.

> **Exam mental trigger:**
> If the scenario says **live**, **real-time**, **instant**, **sub-second**, **event-driven**, **leaderboard**, **anti-cheat**, **fraud detection** â†’ youâ€™re designing a streaming pipeline and the first question is **â€œwhat is my broker?â€**

---

## 2) ğŸ§­ Streaming use cases (the exam is testing â€œrecognise the patternâ€)

The exam usually doesnâ€™t ask â€œwhat is streaming ETL?â€ â€” it asks a scenario and you must map it fast.

### A) **Simple Streaming ETL (Analytics fast path)**

**What it really means:** you mostly want to land events in BigQuery quickly with minimal work.

* **Broker:** Pub/Sub (common) / Kafka
* **Processing:** Dataflow optional (if tiny transforms)
* **Sink:** BigQuery
* **BI:** Looker

**Good fit when:**

* events are already structured/semi-structured
* you donâ€™t need heavy joins/windowing/state

**Exam cue:** â€œingest clickstream, query immediately in BigQuery, minimal transformationâ€

---

### B) **Complex Streaming ETL (Make it query-ready before BigQuery)**

**What it really means:** BigQuery is **schema-on-write** (it enforces schema), so you often need a processing layer to clean/enrich before landing.

* Join with reference data
* Validate, filter, normalise
* Aggregate in windows (per minute/session/lap)
* Reshape schema

**This is where Dataflow becomes the â€˜heartâ€™.**

**Exam cue:** â€œcompute real-time KPIsâ€, â€œenrich with metadataâ€, â€œsession statsâ€, â€œaggregation per key/timeâ€

---

### C) **Streaming AI/ML (Online inference)**

**What it really means:** inference must happen *in the moment*, not after batch loads.

Typical shape:

* preprocess events (feature extraction)
* enrich with feature store
* call model endpoint (Vertex AI)

**Exam cue:** â€œreal-time scoringâ€, â€œdetect cheating instantlyâ€, â€œfraud detection at transaction timeâ€

---

### D) **Streaming Applications (Serving layer)**

**What it really means:** analytics storage (BigQuery) is not the same as **low-latency serving**.

* BigQuery = analytic queries (seconds, scans)
* Bigtable = serving reads (milliseconds), key-based access

So for â€œlive leaderboards,â€ you often see:

* stream updates â†’ Bigtable (serve app)
* also write to BigQuery (analytics/history)

**Exam cue:** â€œsub-10ms readsâ€, â€œserve live leaderboardâ€, â€œoperational app needs latest stateâ€

---

### E) **Reverse ETL (Activate analytics back into ops)**

**What it really means:** data starts in BigQuery and gets pushed outward.

* segments
* triggers
* operational profile updates

**Exam cue:** â€œpush BigQuery insights into CRM/marketing system / operational DBâ€

---

## 3) ğŸ“¬ Pub/Sub fundamentals (with the â€œreal meaningâ€)

Pub/Sub is built to be a **serverless messaging backbone** that decouples producers and consumers.

### 3.1 The objects (and why they exist)

* **Topic** = named channel where events are published
* **Subscription** = a *delivery configuration* (and effectively a queue) attached to a topic
* **Subscriber** = code/service that receives messages

Why subscriptions matter:

* multiple subscriptions = **fan-out** (same events to multiple systems)
* each subscription can have its own:

    * delivery model (push/pull/BQ/GCS)
    * filtering rules
    * dead-letter policy
    * ack behaviour

This is the â€œdecouplingâ€ value: producers donâ€™t care who consumes.

---

### 3.2 Delivery methods (push/pull/BQ/GCS) â€” the decision logic

#### âœ… Pull subscription

* subscriber requests messages when ready
* best when you need **backpressure control**
* typical for Dataflow streaming pipelines consuming Pub/Sub

**Use when:** you want control + high throughput + stable processing.

#### âœ… Push subscription

* Pub/Sub pushes to an HTTP endpoint
* best for serverless triggers (Cloud Functions / Cloud Run)

**Use when:** â€œtrigger this service when message arrivesâ€.

#### âœ… BigQuery subscription

* Pub/Sub writes messages directly to a BigQuery table

**Use when:** the main need is **simple ingestion into BigQuery without running Dataflow**.

**But**: if you need complex validation/enrichment/windowing/exactly-once pipeline behaviour, you typically move to **Pub/Sub â†’ Dataflow â†’ BigQuery**.

#### âœ… Cloud Storage subscription

* Pub/Sub writes to GCS files

**Use when:** you want to land raw streams as files (later batch/ML/lake usage).

---

### 3.3 Distribution patterns (straight-through, fan-in, fan-out) â€” why the exam cares

* **Straight-through:** simplest â€œone pipelineâ€
* **Fan-in:** multiple producers publish to one topic (central ingestion point)
* **Fan-out:** multiple subscribers consume the same topic (dashboards + ML + storage)

![img_1.png](img_1.png)
> **Exam reason:** This is exactly how Pub/Sub helps you build systems where you can add consumers later without touching producers.

---

## 4) ğŸ›¡ï¸ Pub/Sub features (what turns â€œqueueâ€ into â€œplatformâ€)

### 4.1 Integration features (what theyâ€™re really telling you)

* **BigQuery subscription** = fast analytics ingestion path
* **Dataflow** = heavy-duty processing and stronger semantics
* **Cloud Functions** = event-driven glue/automation
* **SMT (Single Message Transforms)** = lightweight tweaks without a full pipeline
* **Import topics** = managed ingestion from other clouds / Kafka / etc.

The hidden exam idea:

> Pub/Sub often plays â€œfront door,â€ but **Dataflow** is where you implement correctness at scale.

---

### 4.2 Reliability features (this is exam gold)

#### âœ… At-least-once delivery (default)

Meaning: Pub/Sub can deliver the same message more than once if acking fails or the subscriber crashes.

So as an engineer you assume:

* duplicates can happen
* your processing should tolerate them (idempotency or dedupe downstream)

#### âœ… Exactly-once delivery (optional, pull subscriptions; single-region nuance)

The module states Pub/Sub can provide exactly-once delivery for pull subscriptions by using a dedup layer.

**Critical distinction (exam trap):**

* **Exactly-once delivery** means â€œPub/Sub wonâ€™t redeliver after successful ackâ€
* It does **NOT automatically mean** â€œexactly-once processingâ€ end-to-end

Because processing can fail:

* after you change state but before you ack
* or after you write to a sink but before ack
* or sink can be non-idempotent

So you still design carefully.

#### âœ… Ordering keys

Pub/Sub ordering is not â€œglobal ordering for the entire topic.â€
Itâ€™s ordering **within the same ordering key**.

Tradeoff: ordering can reduce parallelism / add latency.

#### âœ… Filtering

Subscriptions can filter by attributes.
This is a scaling/cost trick: donâ€™t deliver irrelevant messages to a consumer.

#### âœ… Dead-letter topics (DLTs)

If a subscriber keeps failing a message, Pub/Sub can send it to a dead-letter topic instead of retrying forever.

**This is the streaming equivalent of a DLQ** in batch pipelines.

---

## 5) ğŸ§± Kafka essentials (and why it exists as a different â€œshapeâ€)

Kafka is designed as a **distributed commit log**.

### 5.1 The key concept: partitions

A Kafka topic is split into **partitions**:

* each partition is an ordered, immutable log
* order is guaranteed **within a partition**
* partitions are how Kafka scales throughput

### 5.2 Consumer groups (parallelism model)

If you have a consumer group:

* partitions are assigned to consumers
* each partition is consumed by only one consumer in the group
* gives you scaling + fault tolerance

### 5.3 Offsets (replay is native)

Consumers track offsets. This makes replay extremely natural:

* â€œstart from offset Xâ€
* â€œrewind and reprocessâ€

Thatâ€™s why Kafka is described as a persistent log that can retain data â€œforever.â€

---

## 6) âš”ï¸ Pub/Sub vs Managed Kafka (deep decision rules)

![img_2.png](img_2.png)

### Pick **Pub/Sub** when:

* youâ€™re building a **new GCP-native** system
* you want **serverless/no-ops**
* you want easy integration with Dataflow/BigQuery/Functions
* you want flexible fan-out patterns quickly

### Pick **Managed Kafka** when:

* you already have Kafka producers/consumers/connectors
* you want **portability** (hybrid/multi-cloud)
* you need a **long-lived event log** with replay by offset
* your org is invested in Kafka tooling

![img_3.png](img_3.png)
> **High-yield exam phrase mapping:**

* â€œno-ops serverless ingestionâ€ â†’ Pub/Sub
* â€œexisting Kafka workloads, minimal code changesâ€ â†’ Managed Kafka
* â€œreplay from historical offset / persistent logâ€ â†’ Kafka
* â€œfan-out decoupling to many GCP servicesâ€ â†’ Pub/Sub

---

## 7) ğŸ”— Hybrid bridging (Kafka Connect) â€” what it buys you

You use managed Kafka Connect when:

* part of your world is Pub/Sub (cloud-native)
* part of your world is Kafka (legacy/hybrid)

Connector types:

* **Source**: Pub/Sub â†’ Kafka (events land in Pub/Sub but legacy Kafka processors need them)
* **Sink**: Kafka â†’ Pub/Sub (Kafka pipeline exists, but you want GCP-native consumption like Dataflow/BigQuery)

This is a very â€œenterprise-realisticâ€ architecture.

---

## 8) ğŸ§ª Lab deep recap (what itâ€™s REALLY teaching)

### The pipeline

1. `esports-simulation.py` generates synthetic events â†’ publishes to Pub/Sub
2. `esports-pipeline.py` launches Dataflow streaming job:

    * reads from Pub/Sub subscription
    * transforms events
    * writes to BigQuery tables:

        * `raw_events`
        * `player_score_updates`
        * `team_score_updates`
3. BigQuery views compute live leaderboards

### The SQL pattern (super testable)

They compute **latest state per entity** using:

* `ROW_NUMBER() OVER (PARTITION BY player_id ORDER BY last_updated DESC) = 1`

Then rank using:

* `RANK() OVER (ORDER BY total_score DESC)`

This pattern shows up everywhere (latest per key, slowly changing-ish streaming state snapshots).

---

### Why the WARNING happened (soft delete)

Dataflow uses GCS for:

* staging artifacts
* temp files
* shuffle/intermediate pieces

If your bucket has retention/soft-delete features, you can get:

* warnings
* unexpected costs

**Exam takeaway:** â€œchoose the right bucket policy for temp/staging.â€

---

### Why the ERROR happened (service agent + SA permissions)

Dataflow is a managed service. It uses:

* a **Dataflow service agent**
* a **worker service account**

If the service agent cannot â€œact onâ€ / access the worker SA, Dataflow canâ€™t start workers.

Lab fix: grant **Cloud Dataflow Service Agent** role on the compute worker SA.

**Exam takeaway:** Dataflow failures at start are often **IAM/service agent** misconfigurations.

---

## 9) âœ… Quiz answers

### Quiz 1 â€” Q1: Pub/Sub advantages (select all that apply)

âœ… Correct:

* **Integrates with many GCP services without extra connectors**
* **Fully managed â€œno-opsâ€ (no cluster management)**
* **Completely serverless + pay-for-what-you-use**

âŒ Incorrect:

* **Long-term message persistence with replay from any historical offset** â†’ Kafka advantage
* **Strict ordering across all messages in a topic without throughput impact** â†’ wrong; Pub/Sub ordering is by ordering key and can affect latency/parallelism

---

### Quiz 1 â€” Q2: Key benefit of Managed Service for Apache Kafka

âœ… Correct:

* **Automates complex operational tasks like broker sizing and rebalancing**

âŒ Incorrect:

* â€œNo Kafka API knowledge neededâ€ â†’ you still use Kafka concepts
* â€œCompletely serverless with no cluster configâ€ â†’ not how itâ€™s positioned

---

### Quiz 1 â€” Q3: Pub/Sub exactly-once processing without idempotent logic

âœ… **False**

Reason:

* Pub/Sub can provide **exactly-once delivery** in specific conditions
* but **end-to-end processing** can still duplicate if failures occur around acking/writing
* so you still design idempotent/duplicate-tolerant processing in real pipelines

> This is one of the most common â€œwording trapsâ€ on the exam: delivery vs processing.

---

## 10) ğŸ”¥ The exam â€œdecision spineâ€ (memorise this)

* **Need cloud-native ingestion + no ops + fan-out** â†’ **Pub/Sub**
* **Need Kafka portability + offsets + long replay** â†’ **Managed Kafka**
* **Need complex transformations / enrichment / windowing** â†’ **Dataflow**
* **Need analytics querying + dashboards** â†’ **BigQuery**
* **Need low-latency serving state (leaderboards)** â†’ **Bigtable**
* **Need poison message handling** â†’ **Dead-letter topic**
* **Need ordering** â†’ Pub/Sub ordering keys or Kafka partitioning strategy