# ğŸ§  Module â€” Bigtable: The Operational Serving Layer (Low-latency apps, schema design, reverse ETL, monitoring, Lab + Quiz)

In the streaming pipeline youâ€™ve built so far, you already have:

* **Pub/Sub** = high-throughput â€œfront doorâ€ ingestion
* **Dataflow** = real-time processing brain (event time, windows, state)
* **BigQuery** = analytical engine (ad-hoc SQL, historical analysis)

Bigtable enters when the question changes from **â€œanalyseâ€** to **â€œserveâ€**:

âœ… *â€œHow do I power live leaderboards, in-game features, or real-time APIs with single-digit ms latency and millions of lookups?â€*
â†’ Thatâ€™s operational serving. Thatâ€™s **Bigtable**.

---

## 1) ğŸ¯ Why Bigtable exists in the pipeline

### The problem BigQuery cannot solve well

BigQuery is excellent for analytics, but it is not designed to be a **hot path** database for an application that needs:

* **single-digit millisecond** reads,
* **millions of reads per second** (RPS),
* predictable latency under spikes,
* key-based lookups like `get(user_id)` / `get(game_id)` / `get(leaderboard_id)`.

BigQuery can return results fast for SQL, but not as a â€œserve every request in < 20msâ€ operational store at massive QPS.

### Bigtableâ€™s role

Bigtable is the **serving database** for â€œalready-computedâ€ results:

* store **pre-materialised** leaderboards, user features, recent history,
* enable instant retrieval by primary key / prefix,
* keep latency predictable as traffic grows.

**Mental model:**
BigQuery answers *questions* (SQL scans).
Bigtable answers *lookups* (row-key reads).

---

## 2) ğŸ§© Core concepts: Bigtableâ€™s data model (first principles)

Bigtable is a **wide-column / key-value** store. Itâ€™s optimised around one core primitive:

### âœ… Row key = the primary index

* The **row key** is the main way you retrieve data efficiently.
* Rows are stored **sorted by row key** (lexicographic order).
* The row key design determines:

    * distribution across nodes,
    * hotspot risk,
    * range scan efficiency,
    * whether your â€œtop queriesâ€ are fast or painful.

### Wide-column model basics (what matters for exams and real life)

* A table is sparse: you can have many columns, but most rows have few populated cells.
* Data is grouped into **column families** (think: â€œcoarse grouping for storage/IO characteristicsâ€).
* Within families you have **column qualifiers** (dynamic columns).
* Cells are **versioned by timestamp** (useful for â€œlatest N valuesâ€ patterns).

**Exam reality:** Bigtable schema is not about â€œnormalisationâ€; itâ€™s about designing for the *few access patterns you must guarantee*.

---

## 3) ğŸ†š BigQuery vs Bigtable: exam decision matrix

### âš¡ When to choose Bigtable

Choose **Bigtable** when you see:

* â€œ**single-digit ms** latencyâ€
* â€œ**millions of lookups** / RPSâ€
* â€œserve to a game/app/APIâ€
* â€œonline feature store / real-time predictionsâ€
* â€œIoT/time-series with high ingest + fast recent readsâ€
* â€œneed linear scaling by adding nodesâ€

### ğŸ§  When to choose BigQuery

Choose **BigQuery** when you see:

* â€œad-hoc analysis / unknown questionsâ€
* â€œjoins, group-bys, large scansâ€
* â€œpetabyte analyticsâ€
* â€œBI dashboards over historical dataâ€
* â€œtraining datasets / offline ML preprocessingâ€

### ğŸ” The combined pattern (very common)

* **BigQuery stores history and supports analysis**
* **Bigtable stores operational projections** for apps
  Example: compute leaderboard scores (BigQuery / Dataflow) â†’ push â€œcurrent leaderboard stateâ€ into Bigtable for instant reads.

**If you see**: â€œanalyse in BigQuery but serve in real timeâ€
â†’ **BigQuery + Bigtable serving layer** (reverse ETL pattern).

---

## 4) ğŸ—ï¸ Architectural patterns with Bigtable (and when to use each)

### Pattern A â€” ğŸï¸ Real-time serving layer (â€œmaterialised operational viewâ€)

**Use when:** the app needs fast reads of â€œcurrent stateâ€.

* Store per-game leaderboard rows
* Store per-user â€œrecent actionsâ€
* Store per-team aggregated stats

**Typical flow:**
Pub/Sub â†’ Dataflow (or BigQuery continuous queries) â†’ **Bigtable** â†’ application reads

**Exam trigger:** â€œlive leaderboard / real-time API / fan experienceâ€ â†’ Bigtable.

---

### Pattern B â€” ğŸ§¬ Online feature store for ML serving

**Use when:** a model needs **real-time feature lookup** per event.

* Vertex AI endpoint receives a transaction
* needs user history + recent behaviour under strict latency
* reads features from Bigtable, predicts, returns decision

**Exam trigger:** â€œonline predictionsâ€, â€œfeature lookupsâ€, â€œfraud detection real-time inferenceâ€ â†’ Bigtable.

---

### Pattern C â€” â±ï¸ Time-series ingestion + recent reads

**Use when:** high ingest and you often query â€œlatest valuesâ€ per device/user.

* IoT sensors
* clickstream
* telemetry

Key success factor: **row key design** to avoid hotspots (more below).

---

### Pattern D â€” ğŸ“Š Analytics on Bigtable without moving data (limited use)

The module mentions interacting/analyzing Bigtable data via:

* programmatic access (e.g., HBase client),
* SQL-style interaction / federated analysis from BigQuery (when supported in your setup).

**Exam trap:** This does *not* make Bigtable a replacement for BigQuery.
Bigtable is still not where you do heavy joins and ad-hoc analytics.

---

## 5) ğŸ”‘ Schema design: â€œSpeed through simplicityâ€ (what really decides performance)

### The golden rule

**Design the row key for your top read patterns.**
Because Bigtableâ€™s performance is largely:
**row-key lookup + locality**.

### What breaks in real life: hotspotting

If all writes land in the same key range, one node/tablet gets overloaded.

**Classic mistake (time-series):**
Row key starts with a monotonically increasing timestamp like:
`2025-09-15T10:30:00Z#sensor_123`

Why it breaks:

* new events all go to the â€œendâ€ of the keyspace,
* one region/tablet becomes the hot write target,
* throughput collapses even if you have many nodes.

**Better strategies (conceptual):**

* Start with an entity key (user/device), then time:

    * `sensor_123#<reversed_timestamp>`
* Or add a hashed/salted prefix when you need write spreading.

### Wide-row pattern for â€œlatest N itemsâ€

If you need â€œcustomer profile + last 5 transactionsâ€ under 20ms:

* row key = `customer_id`
* profile in one family
* transactions stored as multiple cells/columns in another family
* fetch the row and request only the latest N cells

This pattern is heavily exam-friendly because it uses:
âœ… single row read
âœ… no scans
âœ… no joins
âœ… stable latency

---

## 6) ğŸ”„ Moving data between BigQuery and Bigtable (reverse ETL operationalisation)

### Why itâ€™s hard

BigQuery is structured/tabular. Bigtable is wide-column.
So you must map:

* *rows/columns* â†’ *row key + families + qualifiers*

### The key design principle

Combine the fields that are used most often in `WHERE` lookups into the **row key** (because the row key is a primary index).

**Exam cue:** â€œoptimise Bigtable lookupsâ€ â†’ design row keys around query predicates.

---

## 7) ğŸ§° Integration points: Dataflow templates and change streams

The module calls out a pragmatic point:

* you *can* write pipelines from scratch,
* but **Dataflow templates** simplify common ingestion/movement tasks.

Specifically mentioned templates processing continuously:

* **Bigtable change streams â†’ BigQuery**
* **Bigtable change streams â†’ Pub/Sub**

**Exam trigger:** If the question is â€œreplicate Bigtable changes into analytics / messaging without writing custom codeâ€ â†’ Dataflow templates.

---

## 8) ğŸ“ˆ Monitoring + troubleshooting (what breaks and how you isolate it)

### Monitoring: what youâ€™re trying to detect

* rising p95/p99 latency,
* uneven load (hot keys/tablets),
* insufficient node capacity for QPS,
* application-level misuse (too many connections, scans).

### Troubleshooting playbook (from the moduleâ€™s flashcards)

1. **Prove whether Bigtable is the bottleneck**

* Temporarily comment out Bigtable reads/writes.
* If performance improves â†’ your Bigtable usage/schema/client pattern is likely wrong.
* If not â†’ bottleneck elsewhere.

2. **Reuse one long-lived connection**

* Opening Bigtable connections is expensive.
* Use one shared long-lived client/connection; it multiplexes across threads.
  **Real-life failure mode:** opening per-request connections â†’ latency spikes + resource exhaustion.

3. **Ensure reads/writes spread across many rows**

* If you hammer a small number of row keys, nodes canâ€™t share the work.
  **Symptom:** throughput doesnâ€™t scale even after adding nodes.

4. **Compare read vs write performance**

* Reads *much faster* than writes can indicate odd patterns like:

    * scanning large key ranges with few matching rows,
    * reading nonexistent keys,
    * poor range scan design.

---

## 9) ğŸ§± Proactive isolation: Bigtable Data Boost (exam-friendly concept)

Bigtable Data Boost is presented as:
âœ… run analytics/batch jobs on production data **without impacting** primary serving traffic.

Core idea:

* isolate batch/analytic compute from serving cluster traffic,
* use serverless compute that reads underlying storage,
* pay only for what you use.

**Exam trigger:** â€œrun analytical/batch workloads on Bigtable data without harming app latencyâ€ â†’ **Data Boost**.

---

# ğŸ§ª Lab â€” Monitor E-sports Chat with Streamlit (Pub/Sub â†’ BigQuery â†’ Gemini â†’ BigQuery continuous queries â†’ Bigtable â†’ Streamlit)

This lab is important because it stitches together several â€œnew-ish exam patternsâ€:

* BigQuery **continuous queries**
* `APPENDS()` TVF for incremental processing
* BigQuery ML **remote model** (Gemini) used inside SQL
* exporting continuously from BigQuery into **Bigtable**
* Bigtable serving used by an app (**Streamlit**)

---

## ğŸ§­ What you build (end-to-end flow)

1. Python generates chat events â†’ publishes to **Pub/Sub**
2. Pub/Sub subscription **writes directly to BigQuery** table `raw_chat_messages`
3. BigQuery remote model (Gemini via Vertex AI) classifies messages
4. Continuous query writes only **unsportsmanlike** into `unsportsmanlike_messages`
5. Another continuous export pushes unsportsmanlike rows into **Bigtable**
6. **Streamlit app** reads Bigtable and presents moderation UI

**Conceptual lesson:** BigQuery can do near-real-time SQL + ML classification, but Bigtable is where you serve results to an interactive app with predictable latency.

---

## ğŸ§± Resources created (high-yield)

* BigQuery dataset: `esports_analytics`
* BigQuery tables:

    * `raw_chat_messages` (streamed from Pub/Sub)
    * `unsportsmanlike_messages` (partitioned by `timestamp`)
* Bigtable:

    * instance `instance`
    * table `unsportsmanlike`
    * column family `messages`
* Pub/Sub:

    * topic `esports_messages_topic`
    * subscription `esports_messages_topic-sub` (delivery: **Write to BigQuery**)
* BigQuery connection for Vertex AI remote models (`Region.esports_qwiklab`)
* BigQuery remote model: `esports_analytics.gemini_model`

---

## ğŸ” IAM / permissions gotchas (these are exam-grade)

### Gotcha A â€” Pub/Sub â†’ BigQuery subscription needs BigQuery permissions

When subscription delivery is â€œWrite to BigQueryâ€, Pub/Sub uses a service account like:
`service-<PROJECT_NUMBER>@gcp-sa-pubsub.iam.gserviceaccount.com`

You must grant it **BigQuery Data Editor** (dataset-level in the lab), otherwise you see errors about missing:

* `bigquery.tables.get`
* `bigquery.tables.updateData`

**Exam trigger:** â€œPub/Sub subscription cannot write to BigQuery tableâ€ â†’ fix IAM for Pub/Sub service account.

---

### Gotcha B â€” Your publisher (compute SA) needs Pub/Sub Publisher

The generator script runs as the compute service account:
`<PROJECT_NUMBER>-compute@developer.gserviceaccount.com`

It must have **Pub/Sub Publisher** on the topic.

**Trap called out explicitly:** do *not* pick Pub/Sub Lite roles.

---

### Gotcha C â€” BigQuery remote model needs Vertex AI permissions

Creating a **BigQuery connection** generates a service account like:
`bqcx-<PROJECT_NUMBER>@gcp-sa-bigquery-condel.iam.gserviceaccount.com`

That principal must have **Vertex AI User**.

**Exam trigger:** â€œBigQuery remote model / connection permission deniedâ€ â†’ grant Vertex AI User to the BigQuery connection service account.

---

## ğŸ§  Continuous queries: why `APPENDS()` is non-negotiable here

BigQuery continuous queries are designed to process **new rows** without rescanning the whole table.

`APPENDS(TABLE raw_chat_messages)` means:

* â€œonly read rows appended since last executionâ€
* avoids repeated full scans
* prevents duplicate reprocessing (in the intended design)

**Exam trigger:** â€œincremental continuous processing in BigQueryâ€ â†’ `APPENDS()` TVF.

---

## ğŸ¤– Gemini classification inside SQL (what the lab teaches conceptually)

The continuous query uses `ML.GENERATE_TEXT()` with a prompt that forces output:

* only `sportsmanlike` or `unsportsmanlike`

Then filters:

* `WHERE ml_generate_text_llm_result = 'unsportsmanlike'`

**Operational gotcha:** LLM output must be constrained; otherwise youâ€™ll get messy text and your filter breaks. This lab avoids that by strict prompt instructions.

---

## ğŸ“¤ Export to Bigtable continuous query: the â€œpriority trapâ€

The export uses:

```sql
EXPORT DATA OPTIONS (format='CLOUD_BIGTABLE', ...)
AS SELECT ... FROM APPENDS(TABLE esports_analytics.unsportsmanlike_messages)
```

You hit an error:

* Bigtable app profile must be **LOW priority**
* default profile is **HIGH priority**

Fix:

* edit Bigtable **Application Profile** â†’ routing priority **Low**

**Why this exists (first principles):**
Youâ€™re running an ongoing export job (an analytic-style pipeline). Bigtable wants that traffic to be deprioritised so it doesnâ€™t harm serving latency.

**Exam trigger:** â€œEXPORT DATA to Bigtable fails due to app profile priorityâ€ â†’ set profile priority to LOW.

---

## ğŸ–¥ï¸ Streamlit serving layer (what matters)

Streamlit app:

* connects to Bigtable
* displays flagged messages
* simulates moderator actions (ban/suspend/dismiss)

**Conceptual lesson:** Bigtable is the operational backing store for user-facing tools and dashboards that need fast, repeated reads.

---

# âœ… Quiz integration (review question + Quiz 4)

## ğŸ§ª Review question: Vertex AI fraud detection + Bigtable (Select all that apply)

**Correct choices:**

* âœ… Bigtable can be used as a low-latency online feature store for real-time predictions.
* âœ… Bigtableâ€™s high write throughput supports massive-scale streaming ingest.
* âœ… Bigtable scales horizontally to maintain low-latency lookups as load grows.

**Incorrect:**

* âŒ â€œBigtable is primarily used for complex SQL queries to train the model.â€
  Training/offline analytics â†’ **BigQuery** (or other analytics systems), not Bigtableâ€™s sweet spot.
* âŒ â€œVertex AI includes a built-in Bigtable instance.â€
  You provision Bigtable separately; Vertex AI does not magically embed it.

**Decision rule:**
If the model needs **online features under strict latency** â†’ Bigtable.
If you need **training datasets / joins / heavy SQL** â†’ BigQuery.

---

# ğŸ§ª Quiz 4: Bigtable

## Q1) Best schema for <20ms: profile + 5 most recent transactions

**Correct answer:** âœ… **Single wide table with row key = customerID**; profile family + transactions family; fetch with one ReadRows and request last 5 cells.

**Why itâ€™s correct (first principles):**

* one primary-key read (fast),
* no scans,
* no joins,
* â€œlatest Nâ€ is handled via cell versions / per-row retrieval controls.

**Why the others are traps:**

* â€œcustomerID#transactionID + scan prefix + limit(5)â€ â†’ âŒ scans are slower and less predictable than a single key read.
* â€œBigQuery then scheduled exportâ€ â†’ âŒ adds latency; not real-time.
* â€œtwo tables profiles + transactionsâ€ â†’ âŒ two reads + application join; more latency and more failure points.

**Exam trigger:** â€œprofile + recent N items under strict latencyâ€ â†’ **one row per entity** pattern.

---

## Q2) Valid/recommended methods to analyse or interact with Bigtable (Select all that apply)

**Correct answers:**

* âœ… Use the native HBase client for programmatic access.
* âœ… Execute a federated query from BigQuery for interactive SQL analysis *when supported/configured*.
* âœ… Serve low-latency requests for user-facing dashboards (last N actions, etc.).

**Incorrect / trap:**

* âŒ â€œPerforming a JOIN with a customer dimension table using the Bigtable SQL interface.â€
  Bigtable is not a join engine. If joins are central, you chose the wrong primary store â†’ **BigQuery**.

**Decision rule:**
If the requirement says **JOIN** / star schema analytics â†’ BigQuery.
If it says **fast keyed lookups** / â€œlast N actionsâ€ â†’ Bigtable.

---

## Q3) Row key for time-series should always start with timestamp

**Correct answer:** âœ… **False**

**Why false:**
Starting with timestamp is a classic hotspot pattern:

* new writes cluster into the same key range,
* load wonâ€™t distribute evenly across nodes,
* throughput and latency degrade.

**Better exam-friendly reasoning:**
Row keys must distribute writes while still supporting your query patterns:

* entity-first keys (device/user) + time component,
* reversed timestamps or salted prefixes when needed.

**Decision rule:**
If writes arrive in time order at high volume â†’ **avoid monotonically increasing prefixes** in the row key.

---

# ğŸ§¾ Bigtable exam decision rules (memorise)

1. **Need single-digit ms lookups at massive scale** â†’ **Bigtable**
2. **Need ad-hoc SQL / joins / scanning historical data** â†’ **BigQuery**
3. **Need real-time serving of analytics results** â†’ compute in BigQuery/Dataflow â†’ **materialise into Bigtable**
4. **Need online ML feature lookup** â†’ **Bigtable** (online store)
5. **Time-series ingest + recent reads** â†’ Bigtable, but row key must avoid hotspotting
6. **Performance issues** â†’ first check schema + connection reuse + hotspot distribution
7. **Need batch/analytics on Bigtable without hurting serving traffic** â†’ **Bigtable Data Boost**
8. **BigQuery continuous queries incremental processing** â†’ use `APPENDS()`
9. **Continuous export to Bigtable fails due to priority** â†’ set Bigtable app profile to **LOW**

---
