# ğŸ§  Module â€” Dataflow Foundations: Beam â†” Dataflow, Portability, Runner v2, Custom Containers, Cross-Language Transforms

This module is *not* teaching you â€œhow to write Beam codeâ€ (thatâ€™s the next course). Itâ€™s teaching you **how to reason about Beam pipelines as portable artifacts**, and how Dataflow fits into that picture as a managed runner that optimises execution at scale.

---

## 1) ğŸ¯ Why this part exists in the pipeline journey

Up to now, you used Dataflow as â€œthe streaming brainâ€ of your architecture. The exam (and real life) then hits you with questions like:

* â€œCan we write in Python but still use the best Kafka connector?â€
* â€œCan we migrate the same pipeline from on-prem to cloud without rewriting?â€
* â€œHow do we control the worker runtime (dependencies, OS libs) safely?â€
* â€œWhy does this pipeline behave differently across runners/languages?â€

This module exists to give you the **mental model** that answers those questions:

### Big idea

âœ… **Apache Beam defines the pipeline** (portable definition).
âœ… **A runner executes it** (Dataflow is one runner).
âœ… **Portability is the mechanism that makes language/runner mixing possible**.

---

## 2) ğŸ§  Core concepts (definitions + mental models)

### ğŸ§© Apache Beam (what it is)

**Apache Beam** is an open-source **unified programming model** for **batch and streaming** pipelines.

* You write a pipeline using a **Beam SDK** (Java, Python, Go, SQL, etc.).
* The *same conceptual classes/abstractions* can represent batch and streaming sources.
* The pipeline definition is independent of where it runs.

**Mental model:** Beam is the *blueprint*.

---

### ğŸƒ Runner (what it is)

A **runner** is the execution engine chosen to run the Beam pipeline.

* You can run locally (for dev), on a VM, or on a managed service.
* Each runner has its own config and backend.

**Mental model:** Runner is the *factory that builds from the blueprint*.

---

### â˜ï¸ Dataflow as a runner (what it adds)

**Dataflow** is Google Cloudâ€™s fully managed runner for Beam pipelines.

From the transcript, the â€œwhy customers value itâ€ is operational, not conceptual:

* automated provisioning/management of resources
* **autoscaling**
* **dynamic work rebalancing** (keeps workers busy, reduces skew impact)
* integrates with Google Cloud logging/monitoring

**Mental model:** Dataflow is the *managed factory* that optimises throughput and cost while you focus on pipeline logic.

---

## 3) ğŸŒ Beam Portability Framework (the real focus)

### What portability is (first principles)

Portability exists because historically:

* Beam pipelines were tied tightly to the SDK language + runner specifics.
* Runners didnâ€™t consistently support all SDKs equally.

Portability solves that by introducing a **language-agnostic representation** of pipelines and standardised communication between:

* **SDKs** (where you author the pipeline)
* **Runners** (where it executes)

### Portability API (the interoperability layer)

The module calls the interoperability layer the **Portability API**:

* well-defined, language-neutral data structures and protocols
* enables â€œSDK of your choiceâ€ + â€œrunner of your choiceâ€
* makes it feasible for â€œevery runner to work with every supported languageâ€ (vision)

**Exam nuance to remember:** portability is not just â€œmarketing about no lock-inâ€ â€” itâ€™s **a concrete interoperability mechanism** (protocols + representation) that makes multi-language execution possible.

---

## 4) ğŸ§± Container environments (why they matter in portability)

Portability depends heavily on **containerisation** because you need a consistent runtime on worker nodes.

### âœ… What containerisation gives you

* **Hermetic worker environment** (isolated from other runtimes)
* You can include **arbitrary dependencies**
* **Ahead-of-time installation** (reduce runtime surprises)
* Each user operation can be associated with an **environment** in which it executes

**Mental model:** Portability is â€œpipeline representation + standard protocolsâ€.
Containers are â€œhow you guarantee the runtime behaves the sameâ€.

---

## 5) ğŸï¸ Dataflow Runner v2 (exam trigger)

The transcript is explicit:

> To use portability features, you must use **Dataflow Runner v2**.

Runner v2:

* uses a more efficient, portable work architecture based on Beam portability
* supports:

    * **custom containers**
    * **multi-language pipelines**
    * **cross-language transforms**
* is packaged with **Dataflow Shuffle service** and **Streaming Engine** (covered next module)

### Decision rule (exam style)

* If the requirement mentions **portability features** (custom containers, multi-language, cross-language transforms) â†’ **Runner v2**.

---

## 6) ğŸ§° Custom containers: what you actually do (and what breaks)

### When you need a custom container

Use it when the default Beam runtime image is not enough:

* system libraries (e.g., specialised compression libs)
* pinned dependency versions
* private wheels / internal packages
* reproducible builds / consistent prod runtime

### Practical steps (what the course expects you to know)

1. Create a **Dockerfile** using the Apache Beam base image as parent.
2. Add your dependencies/customisations.
3. Build the image and push it to a container registry (e.g., `gcr.io`) using:

    * Cloud Build, or
    * Docker CLI
4. Launch the Dataflow job referencing normal params + **custom container image URI**.

### Gotchas (real-life failure modes)

* **Beam SDK version gate:** custom containers require **Beam SDK 2.25.0+** (per transcript).
  *Exam cue:* if they say â€œcustom containers not workingâ€ â†’ check Beam version.
* **Local testing requires Docker installed** (if you want to run locally).
* Registry/permissions issues are common:

    * job can start but workers fail to pull image (missing permissions / wrong registry host / tag).

---

## 7) ğŸ”€ Cross-language transforms (why they exist and how they work)

### The problem they solve

Beam SDKs donâ€™t always have feature parity. Historically:

* some I/O connectors existed only in Java
* Python users were blocked or had to rewrite components

Portability changes that: **a single pipeline can execute transforms written in different languages**.

### What a cross-language transform is

A transform authored in one language (often Java) that can be used from a pipeline authored in another (e.g., Python).

The example in the transcript:

* Python pipeline uses `ReadFromKafka` from `apache_beam.io.kafka`
* but that transform is **implemented in the Java SDK**

### What happens under the hood (high-yield mechanics)

* The Python SDK starts a **local Java service** to create/inject Java pipeline fragments (think â€œexpansion serviceâ€ conceptually).
* It **downloads** the Java dependencies needed to execute the transform.
* At runtime, Dataflow workers execute **Python and Java code simultaneously**.

### Gotchas (what breaks)

* Cross-language transforms can introduce:

    * additional dependency download time
    * version mismatches between SDK/runtime expectations
    * â€œworks locally but fails on Dataflowâ€ if container/runtime differs
* If your pipeline suddenly requires Java artefacts while youâ€™re â€œin Pythonâ€, debugging gets trickier (you must think multi-runtime).

### Decision rule (exam style)

* If the question says: â€œPython pipeline needs a connector only available in Javaâ€ â†’ **cross-language transform via portability**.
* If the question says: â€œneed a controlled runtime for dependenciesâ€ â†’ **custom containers + Runner v2**.

---

## 8) ğŸ§  Exam cheats: pick-the-tool cues + common traps

### High-yield cues

* â€œWrite once, run on any runner / any SDKâ€ â†’ **Beam portability framework**
* â€œLanguage-agnostic representation + protocols between SDK and runnerâ€ â†’ **Portability API**
* â€œCustom containers / multi-language / cross-language transformsâ€ â†’ **Dataflow Runner v2**
* â€œNeed Kafka IO in Python but only Java has the mature connectorâ€ â†’ **cross-language transform**
* â€œNeed hermetic runtime with custom depsâ€ â†’ **containerised Beam environment**

### Common traps

* **Trap:** â€œPortability = hermetic worker environmentâ€
  â†’ Not exactly. Hermetic env comes from **containerisation**, portability is broader (representation + protocols).
* **Trap:** â€œCross-language transforms = portability frameworkâ€
  â†’ Cross-language transforms are a **benefit enabled by portability**, not the definition of the framework itself.
* **Trap:** â€œDataflow = Beamâ€
  â†’ Beam defines; Dataflow executes (runner). Donâ€™t mix blueprint and factory.

---

# âœ… Quiz (integrated with reasoning)

## Q1) Benefits of Beam portability (Select ALL that apply)

Options:

* Cross-language transforms
* Running pipelines authored in any SDK on any runner
* Implement new Beam transforms using a language of choice and utilise these transforms from other languages

âœ… **Correct:** **All of the above**

**Why:**

* Portability enables **cross-language transforms** by standardising representation and environments.
* The vision is â€œ**any SDK on any runner**â€ via the portability interoperability layer.
* Once transforms can be represented language-agnostically, you can implement in one language and **use from others**.

---

## Q2) What is the Beam portability framework? (Single best answer)

Options:

* A set of protocols for executing pipelines
* A hermetic worker environment
* A language-agnostic way to represent pipelines
* A set of cross-language transforms

âœ… **Correct:** **A language-agnostic way to represent pipelines**

**Why this is the best choice:**

* The transcript defines portability as a **language-agnostic way of representing and executing** Beam pipelines, with protocols/structures between SDKs and runners.
* â€œProtocols for executing pipelinesâ€ is *part* of it (Portability API), but the frameworkâ€™s defining feature is the **language-neutral representation**.

âŒ Why the others are wrong:

* â€œHermetic worker environmentâ€ â†’ thatâ€™s primarily **containerisation**, which supports portability but isnâ€™t the definition.
* â€œA set of cross-language transformsâ€ â†’ those are a **capability enabled by portability**, not the framework itself.
