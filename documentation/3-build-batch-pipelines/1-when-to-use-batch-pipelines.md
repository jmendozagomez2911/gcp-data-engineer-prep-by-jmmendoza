# ğŸ¤–ğŸ“¦ **README â€” Module 01: When to choose batch data pipelines**

**Goal:** Know *exactly* when a **batch data pipeline** is the right pattern, what its core components are, which GCP services map to each stage, and which design ideas the **Professional Data Engineer** exam will test.

**Read me like this:**

1. What batch pipelines are â†’ 2) When to choose batch vs streaming â†’ 3) Components + GCP mapping â†’ 4) Batch processing vs pipeline â†’ 5) Core features (throughput, latency, cost) â†’ 6) Cymbal challenges â†’ 7) Ingestion patterns (Cloud Storage hub) â†’ 8) Exam-style patterns + quiz logic â†’ 9) Micro-checklist.

---

## 1) ğŸ§  Mental model: what is a *batch data pipeline*?

**Definition (exam level):**

> A **batch data pipeline** is a *sequence of processes* that **ingests â†’ transforms â†’ sinks** *large, finite datasets* (â€œbatchesâ€) at **scheduled intervals**, optimized for **throughput and efficiency**, not for low latency.

Contrast with streaming:

* **Batch**: works on **bounded** data (e.g. â€œall transactions for 2025-01-01â€), processed in one or more scheduled runs.
* **Streaming**: works on **unbounded**, continuously arriving data (events in real time).

> **Exam tip:** Words like *â€œend of dayâ€, â€œdaily after business hoursâ€, â€œmonthly reportâ€, â€œfive years of historical dataâ€* â†’ very strong batch signal.

---

## 2) ğŸ“Œ When is batch the right pattern? (Use cases)

Typical **batch-friendly** scenarios:

* **Scheduled reports & analytics on historical data**

    * Daily/weekly/monthly KPIs, trend analysis, financial reports.
* **Massive datasets that need heavy transformations**

    * Cleaning, aggregation, joins, business logic over *all* the data.
* **Data warehouse loading / ELT**

    * Periodic loads from OLTP systems, third-party data, logs â†’ **BigQuery**.
* **Bulk data movement**

    * Large volumes from on-prem â†’ cloud or between systems.
* **Backups / archival / DR**

    * Large periodic snapshots of data for recovery/compliance.

> **Exam pattern:**
> â€œProcess **all of the dayâ€™s transactions at once** after close of business; **real-time is not required**; must be **throughput-optimised and cost-effective**â€ â†’ **Batch processing**.

---

## 3) ğŸ—ï¸ Components of a batch pipeline + GCP mapping

Keep this pipeline shape in your head and map each part to GCP services.

### 3.1 Data sources

* Operational DBs, CSV, JSON, log files, third-party APIs, other clouds.
* Often heterogeneous (schemas, formats, frequencies).

### 3.2 Data ingestion (â†’ landing zone)

* **What it is:** Move raw data from sources to a **central staging area**.
* **On GCP:**

    * **Cloud Storage** bucket as **landing / staging** zone.
    * In some cases: direct ingestion into **BigQuery** or from other clouds.

> **Architectural best practice:**
> **Land raw data in durable storage before transforming**. This **decouples ingestion from processing** and lets you re-run failed jobs from the raw data.

â¡ **Quiz #1 principle:**
Correct answer: *â€œBecause it decouples ingestion from processing, allowing the transformation job to be re-run from the raw source data if it fails.â€*

---

### 3.3 Data transformation

* Clean, validate, enrich, join, aggregate, map to canonical schemas, apply business rules.
* **On GCP:**

    * **Dataflow (Apache Beam)** â€” unified model for **batch & streaming**.
    * **Dataproc Serverless for Apache Spark** â€” run Spark code without managing clusters.

> **Exam tip:** Team with existing **Spark** jobs and want serverless â†’ **Dataproc Serverless (for Spark)**, **minimal code changes**.
> (Quiz #10 principle.)

---

### 3.4 Data sink (final storage)

* Where transformed data ends up for analytics / downstream use.

Common GCP sinks:

* **BigQuery** â†’ analytical DWH for interactive SQL over massive datasets.
* **Cloud Storage** data lake (often with table formats like **Iceberg**, etc.).
* Other analytical stores depending on use case.

---

### 3.5 Downstream uses (not â€œpipelineâ€, but very examinable)

Examples (Cymbal Superstore):

* **Financial reporting** in BigQuery.
* **BI dashboards** on historical data.
* **ML models** using years of cleaned transaction data to forecast sales.

---

### 3.6 Orchestration & monitoring (wraps the whole pipeline)

* **Orchestration:** order, dependencies, schedules, retries.

    * On GCP: **Cloud Composer** (Airflow), **Workflows**, **Cloud Scheduler**.
* **Monitoring & observability:** health, errors, performance, SLAs.

    * On GCP: **Cloud Logging**, **Cloud Monitoring**, alerts, dashboards.

> **Quiz #8 principle:**
> â€œFailure discovered hours later, hard to find errorsâ€ â†’ challenge is **Reliability & Observability**, solved by **centralized logging and metrics-based monitoring**.

---

## 4) ğŸª Cymbal Superstore: concrete mapping to GCP

Learn this story as a template:

1. **Sources:** CSV + JSON billing data from many systems.
2. **Ingestion:** Automated landing into **Cloud Storage** (central staging).
3. **Transformation:**

    * Use **Dataflow for Apache Beam** *or* **Serverless for Apache Spark**
    * Read raw data from Cloud Storage â†’ clean, validate, standardize.
4. **Sink:** Write cleansed, structured data to **BigQuery** (enterprise warehouse).
5. **Downstream:**

    * Financial reports, dashboards, ML on historical sales.
6. **Orchestration & monitoring:**

    * **Cloud Composer** schedules jobs.
    * Logging + monitoring to ensure reliability and data quality.

> **Exam tip:** If the case mentions **â€œmillions of daily transactionsâ€, â€œfinancial reconciliationâ€**, and **no real-time need**, plus GCP services like **Cloud Storage + Dataflow/Dataproc + BigQuery**, they are describing **exactly this pattern**.

---

## 5) ğŸ§¬ Batch *processing* vs batch *data pipelines*

You must be able to distinguish the terms:

| Term                      | Focus                                                                                                                                 |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| **Batch data processing** | The **method**: collect data in a **batch** and process it in a scheduled run.                                                        |
| **Batch data pipeline**   | The **end-to-end system** that **implements** batch processing (sources â†’ ingestion â†’ transform â†’ sink â†’ orchestration & monitoring). |

> In casual speech theyâ€™re mixed, but exam questions may talk about **â€œbatch processing featuresâ€** vs **â€œpipeline architectureâ€**.

---

## 6) âš™ï¸ Core features of batch data processing (what exam loves)

Batch processing normally implies these **four properties**:

1. **Scheduled & automated**

    * Jobs run **on a schedule** (nightly, hourly, monthly) with **no manual intervention**.
2. **High throughput**

    * Optimised for **processing huge volumes** efficiently (terabytes, years of history).
3. **Latency-tolerant**

    * Accepts **higher latency** (e.g. results ready next morning) in exchange for efficiency.
    * Ideal for **bounded historical workloads** (e.g. 5 years of sales).
4. **Resource optimisation (burst usage)**

    * Compute can **scale up during the job** then **scale down or shut off**.
    * Avoids paying for always-on infrastructure.

> **Quiz #2 principle:**
> Historical 5-year dataset for model training â†’ key idea is: **batch is designed for very large, bounded datasets**.

> **Quiz #4 principle:**
> â€œComplex validations across **entire dayâ€™s data at once**â€ â†’ relies on **operating on a complete, bounded dataset**.

> **Quiz #6 principle:**
> CFO wants to cut costs; job runs 4h/day â†’ serverless batch **only charges during execution** (resource optimisation).

---

## 7) ğŸ›°ï¸ Serverless & cost: why the business cares

When the exam says **â€œfully serverlessâ€**, think like this:

* You **donâ€™t manage infrastructure** (no cluster sizing, patching, OS updates).
* Platform handles **provisioning, scaling, tear-down**.
* You pay **only while jobs run**, not 24/7.

> **Quiz #5 principle:**
> Main business value: **reduces total cost of ownership by shifting operational overhead** to the cloud provider.

> **Quiz #6 principle (again):**
> On-prem cluster running 24/7 vs serverless that runs 4h/day â†’ **direct cost saving** by paying only for active job time.

And for teams with existing Spark:

> **Quiz #10 principle:**
> Most logical move is **â€œadopt managed/serverless that runs existing Spark code with minimal changesâ€** â†’ on GCP, think **Dataproc Serverless for Spark**.

---

## 8) ğŸ“¥ Initial ingestion patterns (Cloud Storage as the hub)

### 8.1 Cloud Storage = central staging layer

Key architectural idea:

* Land **all raw batch files (CSVs, JSON, logs, etc.) into Cloud Storage**.
* Treat this as your **single source of truth** for raw data.

**Why it matters:**

* **Decouples** the data source systems from the processing engine.
* Allows **re-runs** if processing fails (no need to re-pull from source).
* Enables multiple downstream consumers (Dataflow, Dataproc, BigQuery, ML).

> **Quiz #1 principle (repeated):**
> The primary reason this adds resilience is **decoupling ingestion from processing**.

---

### 8.2 Programmatic ingestion (Python example â€“ what you must â€œrecogniseâ€)

```python
from google.cloud import storage

def upload_blob(bucket_name, source_file_name, destination_blob_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}.")
```

You donâ€™t need to memorise line-by-line code, but you must understand:

* **Pattern:** local/system file â†’ **Cloud Storage bucket**.
* Afterwards, **Dataflow / Dataproc Serverless** can read from that bucket.

> **Concept tested:** â€œData is **programmatically landed** in a bucket, then batch jobs pick it upâ€ â†’ standard GCP pipeline pattern.

---

### 8.3 Multi-cloud support (high level)

* GCP can **ingest or process data residing in other clouds** without always copying everything first.
* For the exam, the key idea is: GCP can operate in **multi-cloud** scenarios and still use **Cloud Storage + Dataflow/Dataproc/BigQuery** as processing platform.

---

## 9) ğŸ§© Typical challenges of batch pipelines (Cymbalâ€™s problems)

Know these four buckets; exam scenarios are written around them.

1. **Data Volume & Scalability**

    * Rapid data growth overwhelms legacy systems.
    * Need **auto-scaling** pipelines that cope with spikes (e.g. triple volume during sales events).
    * **Quiz #9 principle:** daily volume triples and fixed pipeline fails â†’ challenge is **Data Volume & Scalability**.

2. **Data Quality & Consistency**

    * Many sources, formats, and schema variations.
    * Need cleaning, validation, standardisation to avoid **incorrect financial reports**.

3. **Complexity & Maintainability**

    * Adding more sources and business logic â†’ messy scripts, hard to debug and evolve.
    * Need **well-designed pipelines** with clear stages & orchestration.

4. **Reliability & Observability**

    * Job failures delay reports; errors hard to find.
    * Need **retries, alerts, centralized logging, metrics**.
    * **Quiz #8 principle:** reliability/observability problem â†’ solved by **logging + monitoring**, not by â€œmore transformation frameworksâ€.

> **Quiz #7 principle:**
> When nightly reconciliation fails across multiple sources, the robust long-term solution is:
> **â€œDesign an automated, end-to-end batch data pipeline that orchestrates collection, cleansing, validation on a nightly schedule.â€**

---

## 10) ğŸ§  Future-proofing: batch now, streaming later

A classic exam pattern:

* Requirement **today**: batch.
* Possible requirement **tomorrow**: streaming (near real-time).

Best design choice:

> **Select a programming model that works for both batch and streaming so you can reuse business logic.**

On GCP this screams **Apache Beam (Dataflow)**.

â¡ **Quiz #3 principle:** correct option is *â€œSelect a programming model that is unified for both batch and streaming.â€*

---

## 11) ğŸ“ Exam-style logic behind the module quiz (quick mapping)

You donâ€™t need the letters, just the *idea*:

1. **Landing raw â†’ resilient**
   â‡’ Decouple ingestion from processing; re-run from raw.
2. **5 years historical data**
   â‡’ Batch is ideal for **large, bounded** datasets.
3. **Future streaming**
   â‡’ Pick **unified batch+streaming model** (e.g. Beam).
4. **Full-day financial validation**
   â‡’ Requires **complete, bounded dataset**.
5. **Fully serverless business benefit**
   â‡’ Lower TCO by shifting **ops overhead** to cloud provider.
6. **CFO & 4h batch on 24/7 system**
   â‡’ **Resource optimisation**: pay only when job runs.
7. **Nightly reconciliation failures**
   â‡’ Build **automated end-to-end batch pipeline**, not ad-hoc scripts.
8. **Failure found late, hard to debug**
   â‡’ **Reliability & Observability** â†’ logging + monitoring tools.
9. **Volume triples, fixed resources fail**
   â‡’ **Data Volume & Scalability** challenge.
10. **Existing Spark team wants serverless**
    â‡’ Use **managed/serverless service that runs Spark with minimal changes** (Dataproc Serverless).

---

## âœ… Micro-Checklist (Module 1 cram)

Before moving on, make sure you can:

* Define a **batch data pipeline** and distinguish it from **streaming**.
* List **key use cases** where batch is clearly better (historical reports, huge bounded datasets, nightly reconciliation, model training).
* Draw the **pipeline stages** and map them to **GCP services**:

    * Sources â†’ **Cloud Storage** (landing) â†’ **Dataflow / Dataproc Serverless** (transform) â†’ **BigQuery / GCS** (sink) â†’ Composer / Logging & Monitoring.
* Explain the **4 core features** of batch processing:

    * Scheduled & automated, high throughput, latency-tolerant, resource-optimised.
* Argue why **landing raw data in Cloud Storage** makes the pipeline more **resilient and re-runnable**.
* Recognise **Cymbalâ€™s four challenge categories**: volume, quality, complexity, reliability/observability.
* Explain the value of **serverless** for cost (no 24/7 clusters; pay-per-use).
* Justify choosing a **unified programming model** (Beam) to future-proof for streaming.

---

Si quieres, en el siguiente mensaje me puedes pasar **el siguiente mÃ³dulo** y sigo construyendo READMEs asÃ­, uno por mÃ³dulo, para que tengas un â€œlibro de notasâ€ listo para repasar antes del examen.
