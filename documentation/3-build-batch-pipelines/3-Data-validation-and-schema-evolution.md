# ğŸ¤–ğŸ§¼ README â€” Module 03: Control Data Quality in Batch Data Pipelines

**Goal:** Build batch pipelines that **donâ€™t break**, **donâ€™t stop because of a few bad rows**, and produce **trusted analytics data** (BigQuery / lakehouse) using an exam-friendly pattern: **validate + cleanse + DLQ + logging + trend analysis** + **schema evolution**.

**Read me like this:** 1) Validation & cleansing â†’ 2) DLQ (Dataflow vs Spark) â†’ 3) Logging + error tables â†’ 4) Schema evolution (additive vs breaking) â†’ 5) Schema-on-write vs schema-on-read â†’ 6) Iceberg schema evolution demo â†’ 7) Exam cheats + quiz traps.

---

## 1) ğŸ§  Core idea: â€œFast is useless if the data is wrongâ€

Cymbal Superstoreâ€™s batch billing data hits classic quality issues:

* Incorrect transaction amounts
* Missing customer IDs
* Duplicate entries (glitches)
* Schema differences across sources
* Address encoding inconsistencies

**Your engineering goal:** build logic that checks **completeness, conformity, consistency, and reasonableness**.

### Data validation vs data cleansing (donâ€™t mix them up)

* **Validation** = *detect* rule violations (nulls, invalid types/formats, negative amounts, etc.).
* **Cleansing** = *fix* or remove issues (trim whitespace, standardise values, default values, etc.).

> **Exam tip:** Validation is about **rules**. Cleansing is about **correction/standardisation**. You almost always do both in batch pipelines.

---

## 2) â˜ ï¸ Dead Letter Queue (DLQ) â€” the standard batch pattern

A **DLQ** is how you handle bad data **without crashing the whole job**.

**Principle:**

* Valid records continue to the main pipeline.
* Invalid records are **routed** to a separate storage location for later review/fix.

> **Exam tip:** If the question says â€œprocess valid records without interruption while isolating invalid onesâ€, the answer is **DLQ**.

### DLQ implementation: Dataflow vs Serverless Spark (know the mapping)

| What you need                | Dataflow (Apache Beam)                                                      | Serverless for Apache Spark                                           |
| ---------------------------- | --------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| Where validation logic lives | **ParDo** with a **DoFn** (custom checks)                                   | DataFrame transformations (e.g., `withColumn`, `when`, `array_union`) |
| Routing mechanism            | **Multiple outputs**: main output + **tagged side output** (`TaggedOutput`) | Build an `errors` column, then **split with `filter()`**              |
| DLQ result                   | Side output PCollection = DLQ                                               | â€œInvalid rowsâ€ DataFrame = DLQ                                        |

> **Exam tip:** In Dataflow, â€œsplit outputs in one passâ€ screams **ParDo + tagged side output**.

---

## 3) ğŸ§¾ â€œRouting isnâ€™t enoughâ€: Logging + Error analytics

DLQ tells you **what** failed (the rows). Mature systems also need:

### A) Error logs = the **â€œWHYâ€**

* Row-level detail: **which rule failed**, what value was wrong, timestamp, identifiers.
* Used for **debugging a specific run**.

**Where it lands:**

* Dataflow templates log details to **Cloud Logging**
* Serverless Spark streams driver/executor logs to **Cloud Logging**

### B) Error tables = the **â€œHOW OFTENâ€**

* Aggregated metrics, usually in **BigQuery**
* Lets you query trends:

    * â€œWhich rule fails most?â€
    * â€œIs invalid country code rising this month?â€

> **Exam tip:** â€œLong-term trend analysis / systemic data quality issuesâ€ â†’ **Error table in BigQuery** (not DLQ, not logs).

### Practical production pattern (what they want you to say)

1. **DLQ in Cloud Storage / Iceberg table**: preserve bad rows.
2. **Cloud Logging**: structured error messages for debugging.
3. **BigQuery error table**: aggregated counts by error type for dashboards/alerts.

---

## 4) ğŸ§¼ Validation & cleansing with Serverless for Apache Spark (the 5-step flow)

**Batch validation pipeline flow:**

1. **Input**: raw lands in GCS or Iceberg â†’ read into a DataFrame
2. **Process**: apply rules + cleansing
3. **Split**: valid vs invalid using an `errors`/`validation_errors` column
4. **Output (curated)**: valid rows â†’ trusted table
5. **Output (invalid)**: invalid rows â†’ DLQ table for review/fix

### Key PySpark functions to recognise

* `when()` â†’ conditional rule application
* `trim()` â†’ cleansing whitespace / blank strings
* `concat()` â†’ append error codes/messages to `validation_errors`
* `filter()` â†’ split valid vs invalid

> **Exam tip:** Prefer **built-in Spark functions**. Avoid Python UDFs for validation (see quiz).

---

## 5) ğŸ§± Schema evolution in batch pipelines (structural changes)

Batch pipelines break when schema changes unless you design for it.

### Two categories you must classify

#### A) Additive (non-breaking)

* Add new nullable columns (most common)
* Usually can be handled automatically (tool features)

#### B) Breaking changes

* Delete columns
* Rename fields
* Change data type (STRING â†’ INTEGER, etc.)
* Needs an architectural strategy to avoid downtime and corruption

> **Exam tip:** â€œAdd columnâ€ â†’ merge/evolve schema.
> â€œRename/type changeâ€ â†’ treat as breaking; use facade pattern.

### Batch constraint (important)

You **cannot update a running batch job**.
Handling schema evolution means: **deploy a new pipeline version** that succeeds on the next scheduled run.

---

## 6) How to handle schema evolution (tool-agnostic strategy)

### A) For additive changes (easy mode)

* **Dataflow templates / Job Builder UI**: enable schema evolution via parameters

    * Write disposition: append/overwrite
    * Schema update option: â€œallow field additionâ€ (conceptually)
* **Serverless Spark templates**: pass something like `--merge-schema=true` so new columns land without failing

> **Exam tip:** If the scenario says â€œnew field added, pipeline should not failâ€, choose **schema merge / allow field addition**.

### B) For breaking changes (safe mode): **Facade View Pattern**

This is the â€œkeep dashboards onlineâ€ pattern.

1. Deploy a new pipeline writing to a **new table** (e.g., `data_v2`)
2. Create/update a **VIEW** that unions old + new (`UNION ALL`)
3. Point all consumers to the **stable view**, not the physical tables

**Why this matters:** Consumers stay stable while you migrate/transform behind the scenes.

> **Exam tip:** â€œRename column, must keep downstream dashboards onlineâ€ â†’ **Facade View Pattern**.

---

## 7) Schema-on-write vs Schema-on-read (exam vocabulary)

### Schema-on-write (warehouse model, e.g., BigQuery)

* Schema enforced **at write time**
* Benefits: high query performance + strong data quality
* Trade-off: ingestion rigidity â†’ you must manage schema changes carefully

### Schema-on-read (lake model, e.g., raw files in GCS)

* Store raw; apply schema **when reading**
* Benefits: flexible ingestion
* Trade-off: slower queries + governance risk (â€œdata swampâ€)

> **Exam tip:** BigQuery is the classic **schema-on-write** story in these courses.

---

## 8) Iceberg schema evolution demo (Serverless Spark)

### Business problem

Data science needs an `OS` column derived from the browser user-agent string.

### Technical approach (what you must recognise)

* Run a **Serverless Spark batch job**
* Execute **Spark SQL** against an **Apache Iceberg** table:

    1. `ALTER TABLE ... ADD COLUMN OS STRING`
    2. `UPDATE ... SET OS = CASE WHEN ... END WHERE OS IS NULL`

**Key point:** Iceberg supports **in-place**, **atomic** schema + data updates (transactional safety).

### Why they mention BigLake/BigQuery

After the job, you can query the updated Iceberg table via BigQuery (registered through BigLake) to validate the new column exists and is populated.

> **Exam tip:** â€œAdd column + backfill in Icebergâ€ â†’ Spark SQL `ALTER TABLE` + `UPDATE`, relying on Icebergâ€™s transactional guarantees.

---

## 9) âœ… Quiz answers + traps (straight from your transcript)

### Q1: Rename a column (breaking), dashboards must stay online

âœ… **Facade View Pattern**

### Q2: Dataflow template â€œCloud Storage Text to BigQueryâ€ + DLQ path

âœ… Catches **Parsing errors** + **Conversion errors**

### Q3: Dataflow split: valid â†’ BigQuery, null IDs â†’ GCS

âœ… **ParDo with main output + tagged side output**

### Q4: Why avoid Python UDFs in Spark validation?

âœ… **Performance**: Python UDFs are slow because Spark must move data between JVM-optimised execution and the Python interpreter row-by-row.

---

## 10) ğŸ§  Exam micro-checklist (memorise this vibe)

* DLQ = keep pipeline running; isolate bad rows
* Dataflow split in one pass = **ParDo + tagged side output**
* Dataflow templates DLQ auto-handles: **parsing + conversion**
* Logs = **WHY** (Cloud Logging)
* Error tables = **HOW OFTEN** (BigQuery trend analysis)
* Schema evolution:

    * Additive â†’ allow field addition / merge schema
    * Breaking â†’ **Facade View Pattern**
* Batch constraint: you canâ€™t patch a running job; you deploy a new version for next run
* Avoid Python UDFs for validation in Spark (perf killer)