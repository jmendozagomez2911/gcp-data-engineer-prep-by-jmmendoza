# ü§ñüèóÔ∏è README ‚Äî Module 02: Design batch pipelines (Dataflow vs Serverless Spark) + Transform at scale

**Goal:** Design **scalable, high-throughput** batch pipelines on Google Cloud, choose the right **processing engine** (Dataflow vs Serverless Spark), and apply **large-scale transformation** patterns (joins, aggregations, enrichment) with the exam‚Äôs favourite bottlenecks and fixes.

**Read me like this:**

1. What ‚Äúpipeline design‚Äù really means ‚Üí 2) Distributed processing mental model + failures ‚Üí 3) High-throughput strategies (batch sizing/partitioning/I/O/resources) ‚Üí 4) Iceberg for reliable throughput ‚Üí 5) Transformations at scale (principles + ops) ‚Üí 6) Dataflow transforms cheat sheet ‚Üí 7) Spark transforms cheat sheet ‚Üí 8) Templates/Job Builder vs code ‚Üí 9) Choosing Dataflow vs Serverless Spark (decision table) ‚Üí 10) Quiz anchors + micro-checklist.

---

## 1) üß† What is *pipeline design*?

**Pipeline design** = the process of conceptualising and structuring workflows to move and process **bounded datasets** efficiently.

* **Bounded dataset**: fixed size, available in full before processing ‚Üí batch.
* **Unbounded dataset**: continuous stream ‚Üí streaming.

Design is about strategic choices so your pipeline is:

* **scalable** (handles growth and spikes)
* **reliable** (fault tolerant, re-runnable)
* **cost-effective** (resource-efficient)
* **fit for purpose** (meets analytical needs)

---

## 2) üß© Distributed processing for scale (the only way Cymbal survives)

Cymbal‚Äôs massive daily transactions overwhelm single servers ‚Üí you need **distributed processing**:

**Mental model:**

1. Large dataset/task
2. Split into **subtasks / chunks**
3. Multiple **worker nodes** process in parallel
4. Job completes when all subtasks succeed
5. Failures are handled via retries/reassignment (when possible)

### Failure nuance (important)

* If **a worker fails**, the system typically **reassigns the task** to another worker.
* If the **subtask itself is faulty** (bad data chunk / invalid definition), you can‚Äôt ‚Äúfix it by moving it‚Äù:

    * log the failure
    * send bad records to a DLQ (dead-letter queue) where appropriate
    * require data curation/human intervention if it‚Äôs a true data issue

### On Google Cloud

* **Dataflow (Apache Beam runner)** and **Serverless for Apache Spark** both do distributed processing automatically (provisioning, scaling, orchestration behind the scenes).

---

## 3) üöÄ Design for high throughput (4 strategies + exam bottlenecks)

High throughput = minimise end-to-end processing time while keeping cost sensible.

### 3.1 Optimal batch sizing

**Problem:** processing is inefficient due to overhead (setup/teardown, metadata ops).

* Too small ‚Üí ‚Äúdeath by a million tiny files‚Äù
* Too large ‚Üí memory pressure, long task times, higher blast radius on failure

‚úÖ **Match item (your quiz):**
**Optimal batch sizing ‚Üí** pipeline inefficient because it processes **millions of tiny individual files**.
Fix: **compact** into fewer, larger files; tune batch sizes.

---

### 3.2 Effective data partitioning (avoid skew)

Partition/shard data by keys (date, customer_id, region) so workers can process in parallel with minimal shuffle.

**Data shuffling** = expensive redistribution across workers.

‚úÖ **Match item (your quiz):**
**Effective partitioning ‚Üí** one worker overloaded processing almost all data for one key (e.g., `region='NA'`), other workers idle.
That‚Äôs **data skew**. Fix: repartition/re-key/salt keys to distribute evenly.

---

### 3.3 I/O optimisation (stop reading what you don‚Äôt use)

Focus: reduce read/write volume and improve scan efficiency:

* Use **columnar formats** (**Parquet**, **ORC**) and compression
* Column pruning: read only needed columns
* Fewer bytes scanned = faster + cheaper

‚úÖ **Match item (your quiz):**
**I/O optimisation ‚Üí** pipeline reads entire **500-column CSV** just to use **5 columns**.
Fix: store in **Parquet/ORC** (columnar), so only those columns are read.

---

### 3.4 Efficient resource utilisation

Maximise worker usage via:

* parallelism tuning
* memory management
* workload balancing
* autoscaling (where supported)

**Service notes:**

* **Dataflow:** dynamic work rebalancing + autoscaling adapt to volume.
* **Serverless Spark:** optimised connectors for BigQuery/Cloud Storage + job-scoped compute.

---

## 4) üßä High-throughput pipelines with Apache Iceberg (throughput ‚â† just speed)

High throughput must be **sustained reliably**. The real throughput killers are:

* job failures
* data corruption / partial writes
* manual cleanup overhead

**Iceberg helps in 3 ways:**

1. **Atomic transactions (all-or-nothing writes)**
   ‚Üí safe retries without corrupting tables or duplicating data (**idempotency-friendly**)
2. **I/O optimisation**
   ‚Üí pruning/metadata, efficient reads at scale
3. **Performance**
   ‚Üí makes lakehouse tables on Cloud Storage practical for large pipelines

---

## 5) üß™ Transform data at scale: what transformations are

Transformations convert raw chaos into usable truth:

* cleansing (errors, missing data)
* standardisation (formats/values)
* deduplication
* aggregation (summaries)
* enrichment (join external/context data)
* restructuring (reshape for downstream needs)

**Core principles (must recognise):**

* **Immutability**: don‚Äôt mutate raw; write new outputs
* **Idempotency**: retries don‚Äôt create duplicates/corruption
* **Schema enforcement** (and plan for evolution)
* **Parallelisation**: design ops to scale

### Quick exam check

If you ‚Äújoin transaction logs with product DB to add product name/category/price‚Äù ‚Üí that transformation principle is:

‚úÖ **Data enrichment**.

---

## 6) üèóÔ∏è Dataflow (Apache Beam): primitives + key transforms

### 6.1 Beam mental model

* **PCollection**: immutable distributed dataset (bounded in batch)
* **PTransform**: operation applied in parallel
* **Pipeline**: DAG of transforms from sources ‚Üí sinks

### 6.2 Key Dataflow transforms (know what each is for)

* **ParDo**: element-wise UDF ‚Üí cleansing/enrichment/type conversions/filtering
* **GroupByKey**: group by key (pre-aggregation; shuffle-heavy)
* **Combine**: aggregation (per key or global)
* **CoGroupByKey**: multi-input join pattern (distributed join)

> **Performance hint:** operations like **GroupByKey** and **CoGroupByKey** often imply **shuffle**, so anticipate bottlenecks and partitioning strategies.

---

## 7) ‚ö° Serverless for Apache Spark: how you implement the same ideas

### 7.1 Spark core concepts (minimal but exam-relevant)

* **DataFrame API** is the modern best practice for structured data (columns like `user_id`, `purchase_amount`)
* Transformations are **lazy**, actions trigger execution
* Execution model: **driver** coordinates, **executors** run tasks on partitions; shuffle splits stages

### 7.2 Common transform patterns (PySpark)

**Cleansing:**

* `filter`, `dropDuplicates`, `withColumn`, `na.fill`

**Enrichment and joins:**

* `df1.join(df2, on="key", how="inner")` (and other join types)

**Aggregations:**

* `groupBy().agg(sum/avg/count/...)`
* window functions: `row_number`, `lag`, `lead`, `rank`

### 7.3 Reuse patterns

* Parameterised Spark apps (submit same app with different inputs/keys)
* Templates (Dataproc/Serverless Spark templates)
* Modular Spark code (separate cleansing/aggregation/join modules)

---

## 8) üß∞ Templates / Job Builder UI vs code (Dataflow & Spark)

You can implement pipelines in multiple ‚Äúlayers‚Äù:

* **Job Builder UI**: form-based deployment of pre-existing designs (less code, more parameters)
* **Pre-built templates**: pipeline already built; you supply I/O paths, join keys, output table, etc.
* **Custom templates**: you write Beam/Spark logic once, package it for reuse, then run with parameters

Why this matters:

* troubleshooting templates requires understanding **what transform is happening under the hood**
* optimisation requires knowing where **shuffle** is implied

---

## 9) ü•ä Choosing the engine: Dataflow vs Serverless for Apache Spark

You‚Äôre choosing between:

* **Dataflow** = Beam unified model (batch + streaming), cloud-native pipeline abstraction
* **Serverless Spark** = managed Spark jobs, best when Spark is already your team‚Äôs language

### Decision rules (exam style)

Pick **Dataflow** when:

* you want a **unified batch + streaming** model (future-proof)
* you‚Äôre building **new** cloud-native pipelines
* you value Dataflow‚Äôs managed scaling + pipeline monitoring UI

Pick **Serverless Spark** when:

* you‚Äôre **migrating existing Spark/Hadoop** workloads with minimal changes
* your team has strong **Spark** skills and wants Spark DataFrames/Spark SQL
* workloads are primarily **batch** + data science/ML in Spark ecosystem

### Comparison table (keep this mental version)

| Dimension           | Dataflow                                         | Serverless for Apache Spark                     |
| ------------------- | ------------------------------------------------ | ----------------------------------------------- |
| Core abstraction    | Apache Beam pipelines (DAG), batch + streaming   | Native Spark apps (DataFrames, Spark SQL)       |
| Best for            | New cloud-native ETL + streaming-ready designs   | Migrating Spark/Hadoop, Spark-heavy orgs        |
| Ops model           | Fully serverless, autoscale, dynamic rebalancing | Serverless job-scoped clusters, no cluster mgmt |
| Engineer experience | Beam primitives (PCollections/Transforms)        | Spark primitives (DataFrames/SQL, Spark UI)     |

---

## 10) üìù Quiz anchors (what they‚Äôre really testing)

1. **Joining two datasets on user_id in Dataflow** ‚Üí **CoGroupByKey**
2. **Write once, run anywhere** model across runners ‚Üí **Dataflow (Beam)**
3. Spark structured columns + filtering/calculations ‚Üí **DataFrame API**
4. Worker node fails in distributed job ‚Üí coordinator **reassigns tasks** to other nodes (fault tolerance)

And the earlier matching question:

* **Data skew** ‚Üí partitioning / re-keying
* **Reading too many columns** ‚Üí columnar formats (Parquet/ORC)
* **Too many tiny files** ‚Üí compaction / optimal batch sizing

---

## ‚úÖ Micro-checklist (Module 02 cram)

Make sure you can do these without thinking:

* Explain distributed batch processing (subtasks, workers, retries)
* Map the 4 throughput strategies to concrete symptoms:

    * skew ‚Üí partitioning
    * 500 cols read for 5 ‚Üí columnar I/O optimisation
    * millions of tiny files ‚Üí batch sizing/compaction
    * poor utilisation ‚Üí parallelism/memory/workload balance
* Define transformations + recognise **enrichment** vs cleansing vs aggregation
* In Dataflow: know what **ParDo / GroupByKey / Combine / CoGroupByKey** do
* In Spark: prefer **DataFrames**, understand lazy transforms vs actions, joins + groupBy
* Choose engine:

    * **Dataflow** = unified model, cloud-native, batch+stream future-proof
    * **Serverless Spark** = migrate Spark, Spark-first teams, batch heavy
* State expected behaviour on worker failure: **task reassignment**, not manual restart

---
