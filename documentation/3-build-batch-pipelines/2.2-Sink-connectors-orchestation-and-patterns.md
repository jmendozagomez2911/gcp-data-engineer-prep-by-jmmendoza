# ğŸ¤–ğŸ”Œ README â€” Module 02 (Part 2): Sinks, Connectors, Orchestration, and Execution Patterns

**Goal:** Choose the right **data sink** (BigQuery vs Cloud Storage vs Cloud SQL), understand **connectors vs templates**, apply **I/O best practices**, and internalise the production pattern: **manual run â†’ orchestrated DAG (Cloud Composer)**. Includes the Spark hourly â€œTop 20 items per cityâ€ reference architecture.

**Read me like this:**

1. Sink selection logic (OLAP vs landing zone vs OLTP) â†’ 2) Connectors vs templates â†’ 3) I/O best practices checklist â†’ 4) Orchestration + DAG mental model â†’ 5) Manual job â†’ automated workflow mapping â†’ 6) Spark hourly pipeline (Iceberg on GCS + BigQuery catalog) â†’ 7) Monitoring & optimisation entry points.

---

## 1) ğŸ¯ Choose the best data sink (exam-perfect mapping)

### Match review (your quiz)

* **A â†’ BigQuery**: terabytes daily + complex BI queries & analytics (**OLAP**).
* **B â†’ Cloud Storage**: durable, low-cost **landing zone** for raw or intermediate files in open formats (**Parquet/Avro**).
* **C â†’ Cloud SQL**: small structured dataset for **low-latency lookups** by a live app (**OLTP**).

### Decision rule (use this every time)

* If the destination workload is **analytics / BI / OLAP** at scale â†’ **BigQuery**
* If you need **durable cheap storage** (raw zone, intermediate outputs, open formats, lake) â†’ **Cloud Storage**
* If you need **transactional serving / OLTP** (app reads/writes, immediate lookups) â†’ **Cloud SQL**

---

## 2) ğŸ§± Connectors vs templates (donâ€™t confuse them)

### Connectors

A **connector** is a pre-built adapter that knows how to:

* authenticate
* read/write
* format data
* talk to a specific service

Think: â€œHow does Dataflow/Spark *connect* to BigQuery / Cloud Storage / JDBC?â€
Answer: **connectors**.

Examples:

* Dataflow I/O transforms like **BigQueryIO**, **TextIO**, **AvroIO**, **ParquetIO**
* Spark connectors like **spark-bigquery-connector**
* JDBC connectors for Cloud SQL (via **Jdbc IO** conceptually)

### Templates

A **template** is a **full job** (a complete pipeline design).
You supply parameters (paths, tables, keys, etc.) and run it.

**Analogy from the lesson**

* Connector = camera lens (a building block)
* Template = whole camera (press the button â†’ full job runs)

---

## 3) ğŸ§  Cloud SQL, BigQuery, Cloud Storage: what theyâ€™re â€œforâ€

### 3.1 Cloud SQL (OLTP sink)

Use Cloud SQL when:

* output dataset is **small to moderate**
* need **immediate, low-latency access** for apps
* structured relational constraints matter (schemas, integrity, ACID)
* serving pattern: web backend / CRM / operational lookups

**Important trade-offs (exam-flavoured)**

* **Connection limits**: too many Dataflow workers / Spark executors can overwhelm it
* **Write throughput**: not built for huge parallel bulk writes like BigQuery
* **Cost**: expensive for large analytical storage vs BigQuery/GCS

### 3.2 BigQuery (OLAP sink)

BigQuery is the default â€œsink for batch analyticsâ€ because:

* bulk loading at scale
* serverless + massively scalable
* cost model fits analytics (storage + pay-per-query)

### 3.3 Cloud Storage (landing zone / lake / intermediate)

Cloud Storage is best when you want:

* low-cost durable storage
* flexible formats (raw CSV/JSON, or optimised **Parquet/Avro**)
* staging before loading into BigQuery or serving DB
* open formats to keep flexibility (avoid lock-in)

---

## 4) âœ… I/O best practices (source/sink hygiene)

These are the â€œdonâ€™t break productionâ€ rules:

1. **Optimised formats**

    * Prefer **columnar + compressed + splittable**: **Parquet/Avro**
    * Columnar improves analytics read efficiency; splittable enables parallel reads.

2. **Native connectors**

    * Use built-in / purpose-built connectors (BigQueryIO, spark-bigquery-connector, etc.)
    * Donâ€™t reinvent low-level I/O logic.

3. **Batching & buffering**

    * Group records into **single write operations** to reduce API call overhead.

4. **Schema consistency / validation**

    * Ensure types match the sink schema to avoid ingestion errors.

5. **Error handling**

    * Use **DLQs (Dead Letter Queues)** (Cloud Storage or Pub/Sub) to isolate bad records
    * Keep main pipeline moving with valid data.

6. **Monitor I/O performance**

    * Track read/write throughput, retries, errors; find bottlenecks early.

---

## 5) ğŸ•¸ï¸ Orchestration (what it actually means)

Orchestration ensures tasks run:

* in the **right order**
* at the **right time**
* with **dependencies**
* with **retries and monitoring**

### DAG mental model (Directed Acyclic Graph)

* **Directed**: one-way flow (ingest â†’ transform â†’ sink)
* **Acyclic**: no loops (clear start/end)
* DAG **tasks** â‰  worker nodes

    * tasks are logical steps
    * workers are compute resources that execute them

### Cloud Composer

**Cloud Composer** = managed **Apache Airflow** and the â€œcentral controllerâ€.
It:

* schedules
* triggers jobs (Dataflow templates or Spark batches)
* retries on failure
* monitors task progress

---

## 6) ğŸ§ª Production pattern: manual job â†’ automated workflow

This is a core pattern (and very examinable):

### Step 1: Verify manually

* **Serverless Spark**: run with `gcloud dataproc batches submit pyspark ...`
* **Dataflow**: run a template from Job Builder UI or `gcloud dataflow jobs run ...`

Goal: validate logic + configs + parameters.

### Step 2: Automate with Cloud Composer

You convert that manual command into a **Composer DAG**:

* DAG definition sets schedule (hourly/daily/etc.)
* a task/operator submits the job
* the taskâ€™s config mirrors the CLI flags (region, jars, version, script path, etc.)

---

## 7) ğŸª Reference architecture: â€œTop 20 items per cityâ€ hourly Spark pipeline

### Business goal

Cymbal wants hourly â€œtrending productsâ€ per city.

### Pipeline requirements

* ingest + join + aggregate + rank
* run hourly
* no manual intervention (Composer)
* open source / avoid vendor lock-in

### Architecture pattern (important nuance)

* sink results to **Apache Iceberg on Cloud Storage** to keep an open, reusable asset
  (BigQuery is also valid if analytics consumption is purely in BigQuery.)

### Script flow (remember this separation)

1. **Configuration**

    * set JAR paths (Iceberg runtime + BigQuery catalog JAR)
    * define catalog properties (BigQuery metastore catalog)
    * define input/output tables
2. **Ingestion**

    * create SparkSession + configs
    * register base tables as temp views
3. **Transformation** âœ… core business logic lives here

    * join orders + order_items (last hour filter)
    * aggregate per city & item
    * rank via `ROW_NUMBER() OVER (PARTITION BY delivery_city ORDER BY ...)`
    * filter top 20
4. **Sink**

    * write output to Iceberg table with `.createOrReplace()`

**Review anchor:**
If asked â€œWhere does join/group/rank happen?â€ â†’ **Transformation stage**.

---

## 8) ğŸ” Monitoring & optimisation entry points (where you start debugging)

### Dataflow

* Dataflow UI: job graph + metrics + logs
* **Dataflow Insights**: detects issues like hot keys (data skew) and recommends fixes

### Serverless Spark

* Dataproc UI: logs + job-level metrics
* **Spark UI (SSUI)**: jobs â†’ stages â†’ executors â†’ SQL/DataFrame plans
* Cloud Monitoring Metrics Explorer + Gemini insights (recommendations)

---

## âœ… Micro-checklist (Part 2 cram)

Youâ€™re ready to move on when you can:

* Pick **BigQuery vs Cloud Storage vs Cloud SQL** from a one-line scenario (OLAP vs landing zone vs OLTP)
* Explain **connector vs template** with one sentence
* List 6 I/O best practices (formats, native connectors, batching, schema, DLQ, monitoring)
* Define orchestration as **DAG-based scheduling + dependencies + retries**
* Describe the production pattern: **manual validation â†’ Composer DAG automation**
* Identify â€œTop 20 per cityâ€ pipeline stages and where transformation logic occurs

---
