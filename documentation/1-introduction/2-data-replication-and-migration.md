# ğŸ“¦ğŸšš **README â€” Module 03: Data Replication & Migration**

**Goal:** Pick the **correct ingestion lane** to bring data into Google Cloud based on **data size, bandwidth, and transfer type** (ad-hoc, scheduled, offline, CDC), and understand how **Datastream CDC** works end-to-end.

**Read me like this:** 1) baseline architecture â†’ 2) tool selection by transfer type â†’ 3) STS vs Transfer Appliance vs gcloud storage â†’ 4) Datastream deep dive (CDC event structure + unified types) â†’ 5) lab (Postgres â†’ BigQuery) â†’ 6) exam cheats + quiz mapping.

---

## 1) ğŸ§­ Baseline Architecture (Replicate & Migrate stage)

**Purpose:** bring data from **external/internal systems into Google Cloud** so it can be **transformed** and ultimately **stored/served** in GCP.

**Typical origins**

* On-prem & multi-cloud: file systems, object stores (S3 / Azure Blob), HDFS
* Relational DBs: Oracle, MySQL, PostgreSQL, SQL Server
* Other formats / systems: NoSQL, non-relational sources (often via ETL tooling)

**Where it typically lands**

* **Cloud Storage** (landing zone / data lake)
* **BigQuery** (analytics sink; sometimes direct loads/replication)

---

## 2) ğŸš¦Choose the Right â€œLaneâ€ (transfer types)

Think in **four lanes**:

### A) ğŸ”¹ Ad-hoc online transfer (small to medium)

* Best when: you need to copy data **quickly**, manually or in scripts.
* Tool: **`gcloud storage cp`**
* Destination: **Cloud Storage**

```bash
gcloud storage cp ./file.csv gs://my-bucket/path/
```

> ğŸ’¡ **Exam Tip**
> Keyword â€œ**cp command**â€ + â€œad-hoc transfer to Cloud Storageâ€ â†’ **gcloud storage**.

---

### B) ğŸ”¹ Large online transfer (managed + scheduled)

* Best when: large datasets online, repeated transfers, multi-cloud, HDFS.
* Tool: **Storage Transfer Service (STS)**
* Sources: on-prem, multicloud file systems, object stores (S3, Azure Blob), HDFS
* Destination: **Cloud Storage**
* Key feature: **scheduled transfers** + efficient large-scale movement

> ğŸ’¡ **Exam Tip**
> â€œMove large datasets from S3/Azure/HDFS â†’ GCS and schedule itâ€ â†’ **Storage Transfer Service**.

---

### C) ğŸ”¹ Massive offline transfer (bandwidth-constrained)

* Best when: **very large** datasets and/or **limited bandwidth**
* Tool: **Transfer Appliance**
* Pattern: Google ships hardware â†’ you load data â†’ ship back â†’ data uploaded to GCS

> ğŸ’¡ **Exam Tip**
> â€œVery large dataset + offline/limited bandwidthâ€ â†’ **Transfer Appliance**.

---

### D) ğŸ”¹ Continuous database replication (CDC)

* Best when: ongoing changes from relational DBs to analytics / event-driven use cases
* Tool: **Datastream**
* Sources: Oracle, MySQL, PostgreSQL, SQL Server (and supported GCP relational sources depending on setup)
* Destinations: **Cloud Storage** or **BigQuery**
* Supports:

    * **historical backfill** + **propagate new changes**
    * **selective replication** (schema/table/column)
    * optional processing with **Dataflow** before loading to BigQuery

> ğŸ’¡ **Exam Tip**
> Keywords â€œCDCâ€, â€œnear real-time replicationâ€, â€œWAL/binlog/LogMiner/txn logsâ€, â€œschema/table/column selectionâ€ â†’ **Datastream**.

---

## 3) ğŸ“Š The two factors that decide everything: **data size + bandwidth**

The module explicitly drills this point:

* **1 TB @ 100 Gbps â‰ˆ ~2 minutes**
* **1 TB @ 100 Mbps â‰ˆ ~30 hours**

Decision rule:

* If bandwidth is **good** â†’ online options work (gcloud storage / STS / Datastream).
* If bandwidth is **poor** and data is **huge** â†’ **Transfer Appliance**.

> ğŸ’¡ **Exam Tip**
> If a scenario includes transfer time constraints or slow links, the correct answer is often chosen by this bandwidth logic.

---

## 4) ğŸ§° Toolbox Overview (what each tool is â€œforâ€)

| Need                             | Best tool                            | Why                                                |
| -------------------------------- | ------------------------------------ | -------------------------------------------------- |
| Ad-hoc copy to GCS               | **gcloud storage cp**                | Simple CLI transfer for small/medium datasets      |
| Large online transfer to GCS     | **Storage Transfer Service**         | Managed, efficient, supports scheduled transfers   |
| Massive offline migration        | **Transfer Appliance**               | Avoids slow networks, designed for huge datasets   |
| Continuous DB changes (CDC)      | **Datastream**                       | Near real-time replication from DB logs to GCS/BQ  |
| Full DB migration for apps       | **Database Migration Service (DMS)** | Migrates DB engines into Cloud SQL/AlloyDB/Spanner |
| Complex format/system migrations | **Dataflow templates**               | ETL patterns for non-relational/NoSQL + transforms |

> **Exam nuance:** In this module, **STS is the â€œlarge onlineâ€ file mover**, and **Transfer Appliance is the â€œoffline hugeâ€ mover**. The quizzes strongly reinforce that split.

---

## 5) ğŸŒŠ Datastream Deep Dive (CDC you must know)

### 5.1 What Datastream does

* Enables **continuous replication** from relational DBs into GCP.
* Captures **INSERT / UPDATE / DELETE** by reading the source databaseâ€™s change logs (write-ahead style logs).
* Supports:

    * **Backfill** (historical snapshot) and/or **only new changes**
    * **Selective replication** (schema/table/column)

### 5.2 Where CDC data lands

* **Direct â†’ BigQuery** (analytics)
* **â†’ Cloud Storage** (raw events) and optionally:

    * **Dataflow** for custom processing â†’ then to BigQuery
    * event-driven patterns

### 5.3 Source log mechanisms (recognise names)

* Oracle â†’ **LogMiner**
* MySQL â†’ **Binary Log**
* PostgreSQL â†’ **Logical decoding / WAL**
* SQL Server â†’ **Transaction logs**

> ğŸ’¡ **Exam Tip**
> If the question mentions any of these log systems, it is **screaming Datastream**.

---

## 6) ğŸ§¾ Datastream event message structure (quiz-critical)

Datastream events contain:

1. **Generic metadata**
   Context: source table, timestamps, etc.

2. **Payload** âœ… (**this is the actual data changes**)
   The changed row data in **key-value** format (column â†’ value).

3. **Source-specific metadata**
   Extra origin context: database/schema/table, change type (INSERT/UPDATE/DELETE), system identifiers.

> ğŸ’¡ **Exam Tip (direct quiz hit)**
> â€œActual data changes in key-value formatâ€ â†’ **Payload**.

---

## 7) ğŸ”¢ Unified data types (cross-DB consistency)

Datastream normalises numeric types across databases:

* Oracle `NUMBER`, MySQL `DECIMAL`, PostgreSQL `NUMERIC`, SQL Server `DECIMAL`
  â†’ replicated as **decimal** (unified type)

When it lands:

* **Avro (GCS)** â†’ decimal
* **JSON (GCS)** â†’ number
* **BigQuery** â†’ native **NUMERIC**

Why it matters:

* Consistent typing across heterogeneous sources
* Fewer surprises in downstream processing

---

## 8) ğŸ§ª Lab recap â€” Datastream: PostgreSQL â†’ BigQuery (what you must be able to explain)

### Flow (high-level)

1. Prepare **Cloud SQL for PostgreSQL**
2. Enable logical replication (publication + slot)
3. Create **Datastream connection profiles**
4. Create **stream** (source â†’ destination)
5. Validate replication in **BigQuery**
6. Mutate source data and verify changes appear in BigQuery

### The â€œexamableâ€ configuration details

* Cloud SQL flag: `cloudsql.logical_decoding=on`
* Replication artifacts:

    * **Publication**
    * **Replication slot**
* Stream config:

    * Select schema (`test`)
    * BigQuery dataset location = region
    * Staleness limit set (lab uses **0 seconds**)
* Verify with `SELECT * ... ORDER BY id`

---

## 9) ğŸ§  Decision cheats (memorise)

### Pick the tool

* â€œAd-hoc upload/copy to GCSâ€ â†’ **gcloud storage cp**
* â€œLarge online transfer to GCS, supports schedules, S3/Azure/HDFSâ€ â†’ **Storage Transfer Service**
* â€œHuge dataset + limited bandwidth + offline shippingâ€ â†’ **Transfer Appliance**
* â€œContinuous replication / CDC from relational DB logsâ€ â†’ **Datastream**
* â€œMove entire DB for application migrationâ€ â†’ **Database Migration Service**
* â€œComplex ETL for odd sources/formatsâ€ â†’ **Dataflow templates**

### Pick the landing zone

* Raw files / landing zone â†’ **Cloud Storage**
* Analytics destination â†’ **BigQuery**
* App transactional destination â†’ **Cloud SQL / AlloyDB / Spanner** (depends on app needs)

---

## 10) âœ… Micro-Checklist for the Exam

* Understand **replicate & migrate** stage purpose.
* Choose based on **data size + bandwidth** (the moduleâ€™s key decision axis).
* Know what each tool does:

    * `gcloud storage cp` (ad-hoc)
    * **STS** (large online + scheduled)
    * **Transfer Appliance** (offline massive)
    * **Datastream** (CDC replication to GCS/BQ)
* Datastream internals:

    * Reads DB logs (LogMiner/binlog/WAL/logical decoding/txn logs)
    * Event structure: **metadata vs payload vs source-specific metadata**
    * Unified numeric type mapping
* Lab: explain replication slot/publication + validate changes in BigQuery.

---

## 11) ğŸ“ Quiz mapping (what theyâ€™re testing)

1. â€œActual changes key-value formatâ€ â†’ **Payload**
2. Migration ease influenced by â†’ **Data size + network bandwidth**
3. Very large offline migration â†’ **Transfer Appliance**
4. Tool that uses `cp` ad-hoc to Cloud Storage â†’ **gcloud storage command**
5. Large online transfer from on-prem/multicloud/HDFS to GCS with scheduling â†’ **Storage Transfer Service**