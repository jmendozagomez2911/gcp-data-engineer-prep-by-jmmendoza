# üì¶üöö **README ‚Äî Module 03: Data Replication & Migration **


**Goal:** Choose the **right tool** to bring data into GCP and understand how **CDC** and **bulk transfers** work end-to-end.
**Use this guide:** Read top-down. Skim **Exam Tips**. Copy commands/SQL to practise.

---

## 1) üß≠ Baseline Architecture (Replicate & Migrate stage)

Purpose: **bring data from external/internal systems into Google Cloud** so you can later **transform** and **store** it.

**Where data can come from**

* On-prem & multi-cloud: **file systems**, **object stores** (S3, Azure Blob), **HDFS**, **relational DBs** (Oracle, MySQL, PostgreSQL, SQL Server), **NoSQL**.

Got it üëç You want everything blended into **one seamless narrative**, not split answers. Let me rewrite the whole thing so it flows naturally as a single explanation of **how data lands in GCP**, while also covering **Datastream, Dataflow, DMS, and transfers** together.

---

**üöÄ How Data Lands in GCP**

As a data engineer in GCP, you‚Äôll see multiple entry points for data depending on whether it‚Äôs **bulk transfers**, **scheduled jobs**, or **continuous streaming**.

---

#### üì¶ One-off or Scheduled Transfers

For bulk loads or recurring file transfers, data typically lands in **Cloud Storage** (data lake) or directly in **BigQuery**.

* **Storage Transfer Service** is used to move files (e.g. AWS S3 ‚Üí GCS) at scale, with scheduling options (daily, hourly, etc.).
* **BigQuery Data Transfer Service** covers SaaS apps (Google Ads, YouTube, Campaign Manager), loading them directly into **BigQuery** without staging in GCS.

üí° **Exam tip**: If you see ‚ÄúFiles from S3 to GCS daily,‚Äù the answer is **Storage Transfer Service**.

---

#### üîÑ Continuous Change Data Capture (CDC)

When you need **ongoing sync of databases** into GCP for analytics or replication:

* **Datastream** captures inserts, updates, and deletes from **Oracle, MySQL, PostgreSQL**.
* Data can flow:

    * **Datastream ‚Üí BigQuery** for real-time analytics.
    * **Datastream ‚Üí GCS** (landing raw Avro/JSON) with an optional **Dataflow** step in between for **cleaning, enrichment, or transformations**, before pushing to BigQuery.

üí° **Exam tip**: ‚ÄúContinuous DB changes into BigQuery analytics‚Äù ‚Üí **Datastream ‚Üí BigQuery** (or with Dataflow if transformation needed).

---

#### ‚öôÔ∏è Other Migration Helpers

For broader database moves and complex data pipelines, GCP provides extra tools:

* **Database Migration Service (DMS)** ‚Üí managed service to migrate **MySQL, PostgreSQL, SQL Server, Oracle** databases into Cloud SQL, AlloyDB, or Spanner. Best for **application DB migrations**.
* **Dataflow (Apache Beam)** ‚Üí a fully managed service for both **batch and streaming** data pipelines. Commonly used to:

    * Process streams from **Pub/Sub** or **Datastream**.
    * Transform files in **Cloud Storage** before loading into **BigQuery**.
    * Use **prebuilt Dataflow templates** for formats like Avro, Parquet, or integrations with NoSQL systems.


## 2) ‚öôÔ∏è Toolbox Overview (when to use what)

* **`gcloud storage`** (CLI): ad-hoc **small/medium** copies to **GCS** from local/HDFS/object stores.

  ```bash
  gcloud storage cp ./path/file.csv gs://my-bucket/
  ```

Perfect üëç let‚Äôs enrich that section with a bit more **context and nuance** so it flows naturally with the rest of the ‚Äúhow data lands in GCP‚Äù story. I‚Äôll expand on **when you‚Äôd use each tool**, **extra capabilities**, and tie it to exam-style thinking.

---

#### üì¶ Moving Data into GCP (Transfers & Streaming)

When bringing data into GCP, you‚Äôve got multiple options depending on **volume, frequency, and bandwidth**.

---

#### üîπ Quick / Manual Transfers

* For small or **ad-hoc copies** (e.g., developer moving files from a laptop).
* Tools:

    * **`gcloud storage cp`** ‚Üí CLI for single uploads/downloads.
    * **gsutil** (legacy, being replaced by gcloud) ‚Üí scripting/automation friendly.
* Best for **one-off moves**, **debugging**, or **small datasets**.

---

#### üîπ Storage Transfer Service (STS)

* **Managed service** for **large-scale, online, high-speed transfers**.
* Sources: **on-prem, HDFS, AWS S3, Azure Blob, GCS buckets**.
* Features:

    * Throughput up to **tens of Gbps**.
    * **Incremental sync** (only new/changed files).
    * **Checksums & retries** for reliability.
    * **Scheduling** ‚Üí automate daily/hourly sync jobs.
* Use cases:

    * ‚ÄúSync my S3 bucket to GCS daily.‚Äù
    * ‚ÄúMove hundreds of TBs from on-prem HDFS.‚Äù

---

#### üîπ Transfer Appliance (TA)

* **Physical hardware** shipped to your site.
* Load data onto appliance ‚Üí ship back ‚Üí Google uploads to **Cloud Storage**.
* Sizes: **100 TB, 480 TB, 1 PB+**.
* Best for:

    * **Petabyte-scale migrations**.
    * Sites with **limited bandwidth** or **air-gapped security constraints**.
    * Faster & safer than weeks/months of online transfer.

---

#### üîπ Datastream (CDC ‚Üí GCP)

* **Serverless Change Data Capture** service.
* Sources: **Oracle, MySQL, PostgreSQL, SQL Server, AlloyDB**.
* Destinations: **GCS (raw)** or **BigQuery (analytics)**.
* Features:

    * **Backfill** historical data + **ongoing change capture**.
    * Granular: choose schema, tables, or columns.
    * Secure connectivity (VPC peering, private connectivity).
* Often combined with **Dataflow templates** to:

    * Transform Avro/JSON ‚Üí Parquet.
    * Clean/enrich data before BigQuery.

---

#### üîπ Destination Choices

* **Cloud SQL / AlloyDB** ‚Üí transactional (OLTP) workloads.
* **BigQuery** ‚Üí analytical (OLAP) workloads.
* **Cloud Storage** ‚Üí raw landing zone (data lake).

---

#### üìä Bandwidth Considerations (intuition)

* **1 TB @ 100 Gbps ‚âà 2 minutes**
* **1 TB @ 100 Mbps ‚âà 30 hours**
  üëâ If your link is **fast** ‚Üí STS works well.
  üëâ If your link is **slow** and data is **huge** ‚Üí TA is more efficient.

---

### üí° Exam Tips

* **‚ÄúPetabytes + low bandwidth‚Äù ‚Üí Transfer Appliance.**
* **‚ÄúOngoing daily sync from S3‚Äù ‚Üí Storage Transfer Service with schedule.**
* **‚ÄúContinuous DB changes into BigQuery‚Äù ‚Üí Datastream (with optional Dataflow).**
* **‚ÄúOne-time small copy from laptop‚Äù ‚Üí gcloud storage cp.**

---

## 3) üåä Datastream Deep Dive (CDC)

**What it does**

* Listens to source DB **logs** to capture **INSERT/UPDATE/DELETE** in near real time.
* Sources & their logs:

  * **Oracle** ‚Üí **LogMiner**
  * **MySQL** ‚Üí **Binary Log**
  * **PostgreSQL** ‚Üí **Logical Decoding / WAL**
  * **SQL Server** ‚Üí **Transaction Log**
* Outputs **events** to **GCS** (e.g., **Avro/JSON**) or directly into **BigQuery** tables.
* Can route through **Dataflow** for transformation or event-driven architectures.

**Event structure**

* **Metadata** (generic): source table, timestamps, operation, etc.
* **Payload**: key-value pairs of **column ‚Üí value**.
* **Source-specific metadata**: database/schema/table, change type (e.g., INSERT), source IDs.

Here‚Äôs a clearer, more precise version of that section üëá

---

#### üîπ Unified Data Types

Datastream standardises database-specific numeric types so that downstream systems can handle them consistently:

* **Source normalisation**: Different DBMS types (e.g., Oracle `NUMBER`, MySQL `DECIMAL`, PostgreSQL `NUMERIC`, SQL Server `DECIMAL`) are **normalised to a generic decimal type** during replication.

* **Landing formats**:

    * **Avro (Cloud Storage)** ‚Üí stored as **decimal** (preserves precision and scale).
    * **JSON (Cloud Storage)** ‚Üí stored as **number** (JSON only supports a generic numeric type, no explicit precision).
    * **BigQuery** ‚Üí mapped to native **NUMERIC** (supports up to 38 digits precision and 9 digits scale).

‚û°Ô∏è This approach ensures **cross-database consistency**, so data from heterogeneous sources aligns to a predictable type system without losing precision.

---

**Deployment patterns**

* **Direct to BigQuery** for analytics.
* **Via GCS ‚Üí Dataflow ‚Üí BigQuery** for custom transforms or event fan-out.

> üí° **Exam Tip**
> Keywords ‚Äú**near real-time**‚Äù, ‚Äú**CDC**‚Äù, ‚Äú**replicate Oracle/MySQL/Postgres/SQL Server**‚Äù, ‚Äú**select specific schemas/tables/columns**‚Äù ‚Üí **Datastream**.

---

## 4) üß™ Hands-On Lab (PostgreSQL ‚Üí BigQuery with Datastream)

### A. Prepare Cloud SQL for PostgreSQL

Enable API:

```bash
gcloud services enable sqladmin.googleapis.com
```

Create instance (example flags from lab):

```bash
POSTGRES_INSTANCE=postgres-db
DATASTREAM_IPS=IP_ADDRESS   # region-specific Datastream public IPs
gcloud sql instances create ${POSTGRES_INSTANCE} \
  --database-version=POSTGRES_14 \
  --cpu=2 --memory=10GB \
  --authorized-networks=${DATASTREAM_IPS} \
  --region=REGION \
  --root-password pwd \
  --database-flags=cloudsql.logical_decoding=on
```

Connect & create schema/data:

```bash
gcloud sql connect postgres-db --user=postgres  # password: pwd
```

```sql
CREATE SCHEMA IF NOT EXISTS test;
CREATE TABLE IF NOT EXISTS test.example_table (
  id SERIAL PRIMARY KEY,
  text_col VARCHAR(50),
  int_col INT,
  date_col TIMESTAMP
);
ALTER TABLE test.example_table REPLICA IDENTITY DEFAULT;

INSERT INTO test.example_table (text_col, int_col, date_col) VALUES
('hello',0,'2020-01-01 00:00:00'),
('goodbye',1,NULL),
('name',-987,NOW()),
('other',2786,'2021-01-01 00:00:00');
```

Enable replication artifacts:

```sql
CREATE PUBLICATION test_publication FOR ALL TABLES;
ALTER USER POSTGRES WITH REPLICATION;
SELECT PG_CREATE_LOGICAL_REPLICATION_SLOT('test_replication', 'pgoutput');
```

### B. Create Datastream resources

1. **Connection profiles**

   * **Source (PostgreSQL)**: `postgres-cp` ‚Üí REGION, public IP of `postgres-db`, user `postgres`/pwd `pwd`, DB `postgres`; allowlist IP; **RUN TEST**.
   * **Destination (BigQuery)**: `bigquery-cp` ‚Üí REGION.

2. **Stream**

   * Name: `test-stream`, REGION.
   * Source type: **PostgreSQL** (profile: `postgres-cp`).
   * Replication slot: `test_replication`; Publication: `test_publication`.
   * Select **schema `test`**.
   * Destination: **BigQuery** (profile: `bigquery-cp`), dataset location **REGION**, **staleness limit = 0s**.
   * **Run Validation** ‚Üí **Create & Start**. Wait until **Running**.

### C. Validate in BigQuery

Open **BigQuery Studio** ‚Üí expand dataset `test` ‚Üí table `example_table` ‚Üí **PREVIEW**.
If not visible yet, run:

```sql
SELECT * FROM test.example_table ORDER BY id;
```

### D. Prove CDC (changes flow through)

Connect back to Cloud SQL:

```bash
gcloud sql connect postgres-db --user=postgres   # pwd
```

Apply changes:

```sql
INSERT INTO test.example_table (text_col, int_col, date_col) VALUES
('abc',0,'2022-10-01 00:00:00'),
('def',1,NULL),
('ghi',-987,NOW());

UPDATE test.example_table SET int_col = int_col * 2;
DELETE FROM test.example_table WHERE text_col='abc';
```

Query in BigQuery:

```sql
SELECT * FROM test.example_table ORDER BY id;
```

---

## 5) üß† Choosing the Right Option (decision cheats)

**Dataset size & network**

* **Small/medium, ad-hoc** ‚Üí `gcloud storage cp`
* **Large, scheduled, multi-cloud** ‚Üí **Storage Transfer Service**
* **Massive or low bandwidth** ‚Üí **Transfer Appliance**
* **Continuous DB changes** ‚Üí **Datastream** (CDC)
* **Full DB migration for apps** ‚Üí **Database Migration Service**

**Landing target**

* Analytics (OLAP) ‚Üí **BigQuery**
* App DB (OLTP) ‚Üí **Cloud SQL** / **AlloyDB**
* Pre-process files ‚Üí **GCS** (then Dataflow/Dataproc)

**Processing**

* Simple load/ELT ‚Üí **BigQuery**
* Stream transforms/windows/exactly-once ‚Üí **Dataflow**
* Existing Spark jobs ‚Üí **Dataproc**

---

## 6) ‚úÖ Micro-Checklist for the Exam

* Difference between **`gcloud storage`**, **STS**, **Transfer Appliance**, **Datastream**, **DMS**.
* Bandwidth sizing intuition (1 TB: **100 Gbps ‚âà \~2 min**, **100 Mbps ‚âà \~30 hr**).
* **Datastream**: sources, **log types** (LogMiner/binlog/WAL/txn log), **backfill + ongoing**, **selective replication**, **Avro/JSON**, **direct to BQ** or via **Dataflow**.
* When to land in **GCS vs BigQuery**, and how **CDC** supports near real-time analytics.
* Lab flow: prepare **Cloud SQL**, create **connection profiles**, **stream**, validate in **BigQuery**, then **mutate source** and re-check.

---

### üë©‚Äçüè´ Final Thought

Replication is about **fit-for-purpose movement**: pick the **right lane** (CLI, STS, TA, Datastream), **land** in the right place (**GCS/BQ**), and **prove** end-to-end with queries. If you can **map the scenario to the tool** and **explain why**, you‚Äôre set for the exam.
