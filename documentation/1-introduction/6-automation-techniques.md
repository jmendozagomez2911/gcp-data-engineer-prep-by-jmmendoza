# ğŸ¤–ğŸ“… README â€” Module 03: Automation Techniques (Updated)

**Goal:** Pick the *right* automation style for ETL/ELT pipelines, map it to the correct GCP service, and remember the IAM + â€œserverless vs notâ€ details the exam loves.

**How to read:** 1) patterns â†’ 2) Cloud Scheduler + Workflows â†’ 3) Cloud Composer â†’ 4) Cloud Run functions â†’ 5) Eventarc â†’ 6) Lab recap (Cloud Run â†’ BigQuery) â†’ 7) Exam cheats.

---

## 1) ğŸ§­ Automation patterns you must recognise

Google Cloud automation splits cleanly into **scheduled** vs **event-driven**.

### A) Time-based (scheduled)

* Run on a defined cadence (hourly, nightly, month-end).
* Typical examples:

    * Scheduled ELT: **BigQuery extract/transform + Dataform + load back to BigQuery**
    * Nightly backfills, periodic Spark batches
* Main services:

    * **Cloud Scheduler** (simple cron trigger)
    * **Workflows** (multi-step orchestration, YAML)
    * **Cloud Composer** (Airflow DAG orchestration)

### B) Event-driven

* Run when *something happens*: file upload, message published, audit log emitted, etc.
* Typical examples:

    * â€œGCS object finalised â†’ process â†’ load to BigQueryâ€
    * â€œBigQuery insert/write event â†’ rebuild dashboard / retrain modelâ€
* Main services:

    * **Cloud Run functions** (serverless code execution)
    * **Eventarc** (routes CloudEvents from many sources to targets)

---

### âœ… Core mapping (keep this in your head)

| Trigger / Requirement                             | Smallest correct tool                               | When itâ€™s not enough                                                 |
| ------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------- |
| â€œEvery day at 01:00 run Xâ€                        | **Cloud Scheduler**                                 | If X is multi-step â†’ **Workflows**                                   |
| â€œCall A then B with retries/conditionsâ€           | **Workflows**                                       | If itâ€™s a complex DAG (many tasks/sensors/backfills) â†’ **Composer**  |
| â€œFile uploaded to GCS â†’ run codeâ€                 | **Cloud Run function** (triggered via **Eventarc**) | If heavy Spark/cluster work â†’ function calls **Dataproc**            |
| â€œTrigger on audit/log events (BQ insert/write)â€   | **Eventarc** â†’ Cloud Run/Workflows                  | If you need full downstream DAG orchestration â†’ **Composer**         |
| â€œComplex dependency-rich pipeline across systemsâ€ | **Cloud Composer** (Airflow)                        | If only a few API calls, prefer **Workflows** (serverless + simpler) |

> ğŸ’¡ **Exam Tip**
> Only **Cloud Composer** is *not* serverless here. **Scheduler, Workflows, Cloud Run functions, Eventarc** are serverless.

---

## 2) â° Cloud Scheduler + ğŸ§© Workflows (lightweight scheduled automation)

### Cloud Scheduler (managed cron)

* Automates tasks by invoking workloads at **recurring intervals**.
* You control **frequency** and **time of day**.
* Triggers: **HTTP/S**, App Engine HTTP, **Pub/Sub**, **Workflows**.
* Common usage: â€œScheduled run of a Dataform workflowâ€.

### Workflows (YAML orchestration)

* A **state machine** to call Google APIs in sequence.
* Supports **branching, retries, conditionals**, and structured multi-step jobs.
* Ideal when you need orchestration but donâ€™t want Airflow.

#### Common exam pattern: Scheduler â†’ Workflows â†’ Dataform API (compile + invoke tagged subset)

```yaml
# workflows.yaml (shape)
main:
  params: [projectId, region, repo, tags]
  steps:
  - compile:
      call: http.post
      args:
        url: https://dataform.googleapis.com/v1beta1/projects/${projectId}/locations/${region}/repositories/${repo}:compile
        auth: { type: OAuth2 }
        body: { codeCompilationConfig: { defaultDatabase: projectId } }
      result: comp
  - invoke:
      call: http.post
      args:
        url: https://dataform.googleapis.com/v1beta1/projects/${projectId}/locations/${region}/repositories/${repo}:createWorkflowInvocation
        auth: { type: OAuth2 }
        body:
          workflowInvocation:
            compilationResult: ${comp.body.name}
            includedTags: ${tags}
```

> ğŸ’¡ **Exam Tip**
> â€œLow coding effort + YAML + scheduled trigger + multi-step API chainingâ€ â†’ **Cloud Scheduler + Workflows**.

---

## 3) ğŸ Cloud Composer (Apache Airflow) â€” full orchestration

**Cloud Composer** is the **central orchestrator** when workflows span many systems (GCP, on-prem, multicloud).

* Based on **Apache Airflow**
* Core concepts: **operators**, **tasks**, **dependencies**, **DAG (Directed Acyclic Graph)**
* Features: **triggering, monitoring, logging, retries, error handling**, backfills, sensors
* Dev experience: **Python** DAGs

### â€œShapeâ€ you should recognise

A typical analytics DAG might:

1. pull file from **GCS**
2. load into **BigQuery**
3. run SQL (joins/curation)
4. trigger **Dataproc** for deeper transforms

> ğŸ’¡ **Exam Tip**
> If the question says â€œorchestrationâ€ + â€œdependencies / retries / monitoring / backfillsâ€ â†’ **Cloud Composer**.

---

## 4) ğŸ§© Cloud Run functions (serverless event-driven code)

**Cloud Run functions** execute code in response to events.

* Event sources: **HTTP**, **Pub/Sub**, **Cloud Storage**, **Firestore**, and custom events via **Eventarc**
* Multi-language runtime (good for teams with different stacks)
* Best for: â€œsmall glue codeâ€, API calls, lightweight transforms, triggering Dataproc/Dataflow, loading into BigQuery

### Event-driven ETL pattern (from transcript)

**GCS upload â†’ Cloud Run function â†’ call Dataproc API â†’ run workflow template â†’ output lands in GCS**

---

## 5) ğŸ›°ï¸ Eventarc (event routing layer)

**Eventarc** enables a unified **event-driven architecture**:

* Connects many event sources (Google Cloud services, third-party, custom via Pub/Sub)
* Targets include **Cloud Run functions**, Workflows, etc.
* Uses **CloudEvents** standard format
* Great for â€œless frequent / audit-log drivenâ€ triggers

### High-yield scenario

* **BigQuery insert/write** generates a **Cloud Audit Log event**
* Eventarc captures it and triggers actions such as:

    * rebuild dashboard
    * retrain ML model
    * run a custom pipeline step

> ğŸ’¡ **Exam Tip**
> If the trigger is **Audit Logs** (especially BigQuery events) â†’ **Eventarc**.

---

## 6) ğŸ§ª Lab recap â€” Cloud Run function loads Avro from GCS into BigQuery

### What the lab proves

* **Event-driven ingestion**
* Serverless: file upload triggers compute only when needed
* Uses **BigQuery load job** (good for batch file ingestion)

### Core flow

1. Deploy Cloud Run function (gen2)
2. Trigger on **google.storage.object.finalize**
3. Function loads **Avro â†’ BigQuery table** (autodetect schema)
4. Validate in BigQuery, view logs

### Key code idea (what the exam cares about)

* Event payload provides `bucket` + `name`
* Dataset fixed (e.g., `loadavro`)
* Table derived from filename
* Load options: **AVRO + autodetect + CREATE_IF_NEEDED + WRITE_TRUNCATE**

---

## 7) ğŸ§  Decision matrix (memorise this vibe)

| Need                                                  | Best pick               |
| ----------------------------------------------------- | ----------------------- |
| Simple cron trigger (HTTP/PubSub)                     | **Cloud Scheduler**     |
| Multi-step API workflow with retries/branching (YAML) | **Workflows**           |
| Complex DAG orchestration across many systems         | **Cloud Composer**      |
| Run code on cloud events (serverless)                 | **Cloud Run functions** |
| Route CloudEvents (incl. audit/log) to targets        | **Eventarc**            |

> ğŸ’¡ **Exam Tip (from transcript)**
>
> * Cloud Scheduler = **low coding effort** (config-driven)
> * Cloud Composer = **medium effort** (Python DAG)
> * Cloud Run functions = multi-language
> * Eventarc = language-agnostic routing
> * Only Composer is **not serverless**

---

## âœ… Micro-Checklist for the exam

* Identify **scheduled vs event-driven** triggers.
* Scheduler triggers: **HTTP/S**, **Pub/Sub**, **Workflows**.
* Composer = Airflow: **DAG**, operators, tasks, dependencies, retries, monitoring/logging.
* Cloud Run functions: respond to **HTTP / Pub/Sub / GCS / Firestore / Eventarc**.
* Eventarc: **CloudEvents routing**, especially **Audit Log**-driven automation (e.g., BigQuery writes).
* Remember the quiz definitions:

    * **DAG = Directed Acyclic Graph**
    * â€œCentral orchestratorâ€ â†’ **Cloud Composer**
    * â€œRecurring intervalsâ€ â†’ **Cloud Scheduler**
    * â€œExecute code on eventsâ€ â†’ **Cloud Run functions**
    * â€œUnified event-driven architectureâ€ â†’ **Eventarc**

