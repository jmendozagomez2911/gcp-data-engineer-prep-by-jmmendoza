# ğŸ“˜âœ¨ **README â€” Module 03: The Extract, Load, and Transform (ELT) Data Pipeline Pattern**

* **Goal:** Master ELT on Google Cloud: load first into **BigQuery**, then transform with **SQL scripting / scheduled queries / UDFs / stored procedures / remote functions / notebooks**, and scale workflows with **Dataform**.
* **How to use:** Read top-down. Skim the **Exam Tips**. Copy the examples to practise.

---

## 1) ğŸ§­ What is ELT? (mental model + baseline architecture)

**ELT = Extract â†’ Load â†’ Transform** (transform happens **after** loading into BigQuery).

**Why ELT on GCP**

* âœ… **Simplicity & speed**: land data quickly in BigQuery (**staging**), transform later.
* âœ… **Scale**: push heavy transforms to BigQueryâ€™s engine.
* âœ… **Options**: SQL scripts, scheduled queries, Python/Notebooks, **Dataform**.

**Baseline ELT flow**

1. **Extract + Load** into **BigQuery staging tables**.
2. **Transform in BigQuery** (SQL scripting / scheduled queries / functions / tools like Dataform).
3. **Publish** to **production tables/views** for BI/ML.

> ğŸ’¡ **Exam Tip**
> If the prompt says â€œ*load first, transform in BigQuery*â€ or â€œ*use BigQuery compute for transforms*â€, the answer is **ELT in BigQuery** (often with **Dataform** when workflows get complex).

---

## 2) ğŸ” A common ELT pipeline on Google Cloud

* **Ingestion**: EL/replication tools (e.g., `bq load`, BDTS, Datastreamâ†’BQ/GCS) write to **staging**.
* **Transform**:

  * **BigQuery SQL scripting** (procedural SQL: multi-step logic).
  * **Scheduled queries** (recurring transforms).
  * **UDFs** and **stored procedures** (reusable logic).
  * **Remote functions** (call Python via Cloud Run from SQL).
  * **Notebooks + BigQuery DataFrames** (Python exploration/transforms).
  * **Dataform** (SQL workflow management: dependencies + tests + automation).
* **Publish**: curated **prod** tables/views (with governance).

---

## 3) ğŸ§¾ BigQuery transformation toolkit (scripts, functions, scheduling)

### 3.1 SQL Scripting (procedural SQL)

BigQuery supports a **procedural language** so you can run **multiple SQL statements in sequence** with **shared state**.

* Control flow: `IF`, `WHILE`
* Multi-statement blocks: `BEGIN â€¦ END`
* **Transactions** for integrity
* **Variables** (including system variables)

> ğŸ’¡ **Exam Tip**
> â€œMultiple SQL statements + shared state + IF/WHILE + transactionsâ€ â†’ **BigQuery SQL scripting**.

---

### 3.2 UDFs (user-defined functions)

* **SQL UDFs** (recommended when possible) or **JavaScript UDFs**
* Scope: **temporary** or **persistent**
* JavaScript UDFs can use **external libraries**; community UDFs exist

Use when: you need **reusable transformation logic** across many queries.

---

### 3.3 Stored Procedures

* Encapsulate complex logic as a reusable unit
* Benefits: **reusability**, **parameterisation**, **transaction handling**, maintainability
* Called from apps or within SQL scripts

**Spark stored procedures on BigQuery**

* Can be defined in the BigQuery PySpark editor or via `CREATE PROCEDURE`
* Languages: **Python / Java / Scala**
* Code can be inline or stored in **Cloud Storage**

---

### 3.4 Remote Functions (Cloud Run)

Remote functions let BigQuery call code running in **Cloud Run**.

* Define the remote function in BigQuery (connection + endpoint)
* Use it in SQL like a UDF
* Useful for complex transformations in **Python**

> ğŸ’¡ **Exam Tip**
> â€œCall Python logic from inside BigQuery SQLâ€ â†’ **Remote function + Cloud Run**.

---

### 3.5 Notebooks & BigQuery DataFrames

* Python exploration/transforms over datasets larger than RAM
* Integrates with visualisation libraries
* Can schedule notebook executions (useful for repeatable analysis pipelines)

---

### 3.6 Saved & Scheduled Queries

* Save queries, manage versions, share
* Schedule frequency + start/end times + destination settings

**Limit:** scheduled queries are great for simple jobs, but real pipelines often need post-steps:

* run another SQL script
* run data quality checks
* apply security steps

> ğŸ’¡ **Exam Tip**
> â€œNeed multiple dependent steps + tests + automationâ€ â†’ **Dataform** (not only scheduled queries).

---

## 4) ğŸ§° Dataform (SQL workflow orchestration for BigQuery ELT)

**What it is (clear + high-yield):**
Dataform is a **managed (serverless) tool to organise and run SQL transformations in BigQuery**. You use it when your **data is in BigQuery** (or at least queryable from BigQuery), and you want more than â€œjust running one queryâ€.

It helps you manage, in one place:

* **Transformations** (build tables/views from other tables)
* **Assertions** (SQL-based data quality checks, e.g., â€œno nullsâ€, â€œno duplicatesâ€)
* **Automation** (run the workflow manually or on a schedule)

**How it works (plain):**

1. You write your logic as **SQLX** (SQL + config) and optionally **JavaScript** for reusable patterns.
2. Dataform builds a **dependency graph** (which tables must run first), validates/compiles your code, and shows errors early.
3. When you run it (or schedule it), Dataform triggers **BigQuery jobs** to execute the compiled SQL in the correct order.

âœ… **Key point:** Dataform does **not** replace BigQuery or Spark. It **doesnâ€™t have its own compute**.
Itâ€™s an **orchestrator for BigQuery SQL**: **Dataform plans/runs the workflow; BigQuery does the actual processing.**

---

### 4.1 Repository & workspace structure

* `definitions/` â†’ `.sqlx` definitions (tables/views/incrementals/declarations)
* `includes/` â†’ JavaScript helpers
* Common files: `.gitignore`, `package.json`, `package-lock.json`, `workflow_settings.yaml`, optional `README.md`

---

### 4.2 SQLX file structure

```text
config { ... }          # metadata + materialisation + (optionally) tests
js { ... }              # reusable JS helpers (optional)
pre_operations { ... }  # SQL before main body (optional)
-- main SQL body --
post_operations { ... } # SQL after main body (optional)
```

---

### 4.3 Materialisation types (must know)

* `declaration` â†’ reference an existing BigQuery table
* `table` â†’ create/replace a table from a SELECT
* `incremental` â†’ create then update with new data
* `view` â†’ create/replace a view (optionally materialised)

---

### 4.4 Data quality + custom steps

* **Assertions** (SQL/JS) â†’ data quality tests
* **Operations** â†’ custom SQL before/after/during execution

> ğŸ’¡ **Exam Tip**
> â€œPrimary purpose of assertions?â€ â†’ **data quality tests**.

---

### 4.5 Dependencies (execution order)

* **Implicit**: `${ref("node")}` creates a dependency automatically
* **Explicit**: `config { dependencies: [...] }`
* `resolve()` references without creating a dependency

Workflows are best visualised as a **DAG** (graph of dependencies).

---

### 4.6 Triggers / scheduling

* **Internal triggers**: manual run in Dataform UI or Dataform schedules
* **External triggers**: **Cloud Scheduler** or **Cloud Composer**

Execution still happens **inside BigQuery**.

---

## 5) ğŸ§ª Lab: Create & execute a SQL workflow in Dataform (recap)

### Task 1 â€” Create repository

* BigQuery â†’ Dataform â†’ **CREATE REPOSITORY**
* **ID:** `quickstart-repository` Â· **Region:** `REGION`
* Copy the **Dataform service account**.

### Task 2 â€” Create & init workspace

* Repo â†’ **CREATE DEVELOPMENT WORKSPACE**
* **ID:** `quickstart-workspace` â†’ **INITIALIZE WORKSPACE**

### Task 3 â€” Define a view (`definitions/quickstart-source.sqlx`)

```sql
config { type: "view" }

SELECT "apples" AS fruit, 2 AS count
UNION ALL SELECT "oranges", 5
UNION ALL SELECT "pears", 1
UNION ALL SELECT "bananas", 0
```

### Task 4 â€” Define a table (`definitions/quickstart-table.sqlx`)

```sql
config { type: "table" }

SELECT fruit, SUM(count) AS count
FROM ${ref("quickstart-source")}
GROUP BY 1
```

### Task 5 â€” Grant IAM to Dataform SA

* **BigQuery Job User**
* **BigQuery Data Editor**
* **BigQuery Data Viewer**

### Task 6 â€” Execute workflow

* Workspace â†’ **START EXECUTION** â†’ Execute actions â†’ **START EXECUTION**
* Outputs go to dataset **`dataform`**
* Check **Executions** for logs/status

---

## 6) ğŸ§  Decision cheats (tool picker)

* **Few steps / simple recurrence** â†’ **Saved + Scheduled Query**
* **Complex SQL workflow** (dependencies + tests + operations + incrementals) â†’ **Dataform**
* **Reusable transformation logic** â†’ **UDF**
* **Reusable multi-statement routine with transactions** â†’ **Stored Procedure**
* **Need Python logic callable from SQL** â†’ **Remote function (Cloud Run)**
* **Python exploration / large-scale transformations** â†’ **Notebooks + BigQuery DataFrames**

---

## 7) âœ… Micro-Checklist for the exam

* ELT = load to **BQ staging**, transform **in BigQuery**, publish to **prod**
* BigQuery scripting: multi-statement + shared state + IF/WHILE + transactions + variables
* UDFs: SQL preferred; JS for external libs; temp vs persistent
* Stored procedures: reusable + parameterised + transaction handling; Spark procedures exist (Py/Java/Scala; inline or GCS)
* Remote functions: BigQuery calls Cloud Run from SQL
* Scheduled queries: automate cadence + destination
* Dataform: SQLX structure, materialisations, assertions, operations, dependencies (`ref` / `dependencies` / `resolve`), internal/external triggers; runs in BigQuery

---

### ğŸ‘©â€ğŸ« Teacherâ€™s nudge

If you can (1) define ELT precisely, (2) name BigQueryâ€™s main transformation mechanisms, and (3) explain why Dataform is used for **workflow complexity + data quality + automation**, youâ€™ll cover most exam questions in this module.

---

If you want, I can also produce a **short â€œexam-onlyâ€ version** of this README (1 page) while keeping the same wording and decision rules.
